@article{krizhevsky2012imagenet,
	title={Imagenet classification with deep convolutional neural networks},
	author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	journal={Advances in neural information processing systems},
	volume={25},
	pages={1097--1105},
	year={2012}
}

@misc{crisp1996,
 author = {CRISP-DM},
 year = {1996},
 title = {Cross Industry Process---Data Mining},
 url = {http://www.crisp-dm.org},
 urldate = {2016-08-03}
}

% This file was created with Citavi 6.3.0.0

@article{Shearer2000,
 author = {Shearer, C.},
 year = {2000},
 title = {The CRISP-DM model: the new blueprint for data mining},
 pages = {13--22},
 volume = {5},
 number = {4},
 journal = {Journal of data warehousing}
}

@inproceedings{ares_utility,
	author = {Hittmeir, Markus and Ekelhart, Andreas and Mayer, Rudolf},
	title = {On the Utility of Synthetic Data: An Empirical Evaluation on Machine Learning Tasks},
	year = {2019},
	isbn = {9781450371643},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3339252.3339281},
	doi = {10.1145/3339252.3339281},
	abstract = {With the recent advances and increasing activities in data mining and analysis, the
	protection of the privacy of individuals is crucial. Several approaches address this
	concern, from techniques like data anonymisation to secure, non-disclosive computation,
	all of which have their specific strengths and weaknesses, depending on the specific
	requirements. A slightly different approach is the generation of synthetic data, which
	tries to preserve the overall properties and characteristics of the original data
	without revealing information about actual individual data samples. The promise is
	that, for most purposes, models trained on the synthetic data instead of the real
	data do not show a significant loss of performance. In this paper, we give an overview
	on currently available approaches for synthetic data generation, and empirically evaluate
	the utility of the generated synthetic data by testing them on a number of supervised
	machine learning tasks on several publicly available datasets.},
	booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
	articleno = {29},
	numpages = {6},
	keywords = {Synthetic Data, Machine Learning, Privacy-Preserving Data Mining},
	location = {Canterbury, CA, United Kingdom},
	series = {ARES '19}
}

@article{mice,
	author = {Buuren, Stef and Groothuis-Oudshoorn, Catharina},
	year = {2011},
	month = {12},
	pages = {},
	title = {MICE: Multivariate Imputation by Chained Equations in R},
	volume = {45},
	journal = {Journal of Statistical Software},
	doi = {10.18637/jss.v045.i03}
}

@article {norvig_eod,
	author = {F. Pereira and P. Norvig and A. Halevy},
	journal = {IEEE Intelligent Systems},
	title = {The Unreasonable Effectiveness of Data},
	year = {2009},
	volume = {24},
	number = {02},
	issn = {1941-1294},
	pages = {8-12},
	keywords = {machine learning;very large data bases;semantic web},
	doi = {10.1109/MIS.2009.36},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {mar}
}

@misc{buitinck2013api,
	title={API design for machine learning software: experiences from the scikit-learn project}, 
	author={Lars Buitinck and Gilles Louppe and Mathieu Blondel and Fabian Pedregosa and Andreas Mueller and Olivier Grisel and Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort and Jaques Grobler and Robert Layton and Jake Vanderplas and Arnaud Joly and Brian Holt and Gaël Varoquaux},
	year={2013},
	eprint={1309.0238},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}


@misc{smith_1cycle,
	title={A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay}, 
	author={Leslie N. Smith},
	year={2018},
	eprint={1803.09820},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{goodfellow2014generative,
	title={Generative Adversarial Networks}, 
	author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
	year={2014},
	eprint={1406.2661},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article{mnist,
	title={The mnist database of handwritten digit images for machine learning research},
	author={Deng, Li},
	journal={IEEE Signal Processing Magazine},
	volume={29},
	number={6},
	pages={141--142},
	year={2012},
	publisher={IEEE}
}

@inproceedings{titanic,
	author = {Farag, Nadine and Hassan, Ghada},
	title = {Predicting the Survivors of the Titanic Kaggle, Machine Learning From Disaster},
	year = {2018},
	isbn = {9781450364690},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3220267.3220282},
	doi = {10.1145/3220267.3220282},
	abstract = {April 14th, 1912 was very unfortunate for the most powerful ship ever built at that
	time, the Titanic. Grievously, 1503 out of 2203 passengers perished the sinking, but
	the rationale behind survival still remains a question mark. In efforts to study the
	Titanic passengers; Kaggle, a popular data science website, assembled information
	about each passenger back in the days of the Titanic into a dataset, and made it available
	for a competition titled: "Titanic: Machine Learning from Disaster." This research
	aims to use machine learning techniques on the Titanic data to analyze the data for
	classification and to predict the survival of the Titanic passengers by using data-mining
	algorithms; specifically Decision Trees and Na\"{\i}ve Bayes. The prediction and efficiency
	of these algorithms depend greatly on data analysis and the model. The paper presents
	an implementation which combines the benefits of feature selection and machine learning
	to accurately select and distinguish characteristics of passengers' age, class, cabin,
	and port of embarkation then consequently infer an authentic model for an accurate
	prediction. The data-set is described and the implementation details and prediction
	results are presented then compared to other results. The Decision Tree algorithm
	has accurately predicted 90.01% of the survival of passengers, while the Gaussian
	Na\"{\i}ve Bayes witnessed 92.52% accuracy in prediction.},
	booktitle = {Proceedings of the 7th International Conference on Software and Information Engineering},
	pages = {32–37},
	numpages = {6},
	keywords = {Na\"{\i}ve Bayes, Machine Learning, Decision Trees, Supervised Learning, Kaggle, Data Mining},
	location = {Cairo, Egypt},
	series = {ICSIE '18}
}

@misc{gan_continual_learning,
	title={Generative Adversarial Network Training is a Continual Learning Problem}, 
	author={Kevin J Liang and Chunyuan Li and Guoyin Wang and Lawrence Carin},
	year={2018},
	eprint={1811.11083},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@incollection{catastrophic_forgetting,
	title={Catastrophic interference in connectionist networks: The sequential learning problem},
	author={McCloskey, Michael and Cohen, Neal J},
	booktitle={Psychology of learning and motivation},
	volume={24},
	pages={109--165},
	year={1989},
	publisher={Elsevier}
}

@misc{mode_collapse,
	title={Mode Regularized Generative Adversarial Networks}, 
	author={Tong Che and Yanran Li and Athul Paul Jacob and Yoshua Bengio and Wenjie Li},
	year={2017},
	eprint={1612.02136},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{dcgan,
	title={Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}, 
	author={Alec Radford and Luke Metz and Soumith Chintala},
	year={2016},
	eprint={1511.06434},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{oversampling_gan,
	author = {Suh, Sungho and Lee, Haebom and Jo, Jun and Lukowicz, Paul and Lee, Yong},
	year = {2019},
	month = {02},
	pages = {746},
	title = {Generative Oversampling Method for Imbalanced Data on Bearing Fault Detection and Diagnosis},
	volume = {9},
	journal = {Applied Sciences},
	doi = {10.3390/app9040746}
}

@misc{batchnorm,
	title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
	author={Sergey Ioffe and Christian Szegedy},
	year={2015},
	eprint={1502.03167},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{convolution_arithmetic,
	title={A guide to convolution arithmetic for deep learning}, 
	author={Vincent Dumoulin and Francesco Visin},
	year={2018},
	eprint={1603.07285},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@misc{arjovsky2017wasserstein,
	title={Wasserstein GAN}, 
	author={Martin Arjovsky and Soumith Chintala and Léon Bottou},
	year={2017},
	eprint={1701.07875},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@misc{gulrajani2017improved,
	title={Improved Training of Wasserstein GANs}, 
	author={Ishaan Gulrajani and Faruk Ahmed and Martin Arjovsky and Vincent Dumoulin and Aaron Courville},
	year={2017},
	eprint={1704.00028},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{fan2008liblinear,
	title={LIBLINEAR: A library for large linear classification},
	author={Fan, Rong-En and Chang, Kai-Wei and Hsieh, Cho-Jui and Wang, Xiang-Rui and Lin, Chih-Jen},
	journal={the Journal of machine Learning research},
	volume={9},
	pages={1871--1874},
	year={2008},
	publisher={JMLR. org}
}

@article{vapnik1995support,
	title={Support vector machines},
	author={Vapnik, Vladimir and Guyon, Isabel and Hastie, Trevor},
	journal={Mach. Learn},
	volume={20},
	number={3},
	pages={273--297},
	year={1995}
}

@article{quinlan1986induction,
	title={Induction of decision trees},
	author={Quinlan, J. Ross},
	journal={Machine learning},
	volume={1},
	number={1},
	pages={81--106},
	year={1986},
	publisher={Springer}
}

@article{winequality,
	title = {Modeling wine preferences by data mining from physicochemical properties},
	journal = {Decision Support Systems},
	volume = {47},
	number = {4},
	pages = {547-553},
	year = {2009},
	note = {Smart Business Networks: Concepts and Empirical Evidence},
	issn = {0167-9236},
	doi = {https://doi.org/10.1016/j.dss.2009.05.016},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923609001377},
	author = {Paulo Cortez and António Cerdeira and Fernando Almeida and Telmo Matos and José Reis},
	keywords = {Sensory preferences, Regression, Variable selection, Model selection, Support vector machines, Neural networks},
	abstract = {We propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the certification step. A large dataset (when compared to other studies in this domain) is considered, with white and red vinho verde samples (from Portugal). Three regression techniques were applied, under a computationally efficient procedure that performs simultaneous variable and model selection. The support vector machine achieved promising results, outperforming the multiple regression and neural network methods. Such model is useful to support the oenologist wine tasting evaluations and improve wine production. Furthermore, similar techniques can help in target marketing by modeling consumer tastes from niche markets.}
}

@inproceedings{diabetes,
	title={Using the ADAP learning algorithm to forecast the onset of diabetes mellitus},
	author={Smith, Jack W and Everhart, James E and Dickson, WC and Knowler, William C and Johannes, Robert Scott},
	booktitle={Proceedings of the annual symposium on computer application in medical care},
	pages={261},
	year={1988},
	organization={American Medical Informatics Association}
}

@MISC{bagging,
	author = {Leo Breiman},
	title = {Bagging Predictors},
	year = {1994}
}

@article{conditional_gans,
	author    = {Mehdi Mirza and
	Simon Osindero},
	title     = {Conditional Generative Adversarial Nets},
	journal   = {CoRR},
	volume    = {abs/1411.1784},
	year      = {2014},
	url       = {http://arxiv.org/abs/1411.1784},
	eprinttype = {arXiv},
	eprint    = {1411.1784},
	timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/MirzaO14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{smote,
	title={SMOTE: Synthetic Minority Over-sampling Technique},
	volume={16},
	ISSN={1076-9757},
	url={http://dx.doi.org/10.1613/jair.953},
	DOI={10.1613/jair.953},
	journal={Journal of Artificial Intelligence Research},
	publisher={AI Access Foundation},
	author={Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	year={2002},
	month={Jun},
	pages={321–357}
}

@incollection{mathematics_unreasonable,
	title={The unreasonable effectiveness of mathematics in the natural sciences},
	author={Wigner, Eugene P},
	booktitle={Mathematics and Science},
	pages={291--306},
	year={1990},
	publisher={World Scientific}
}

@book{dataprep,
	title={Data preparation for machine learning: data cleaning, feature selection, and data transforms in Python},
	author={Brownlee, Jason},
	year={2020},
	publisher={Machine Learning Mastery}
}

@article{datawrangling_time,
	author = {Munson, M. Arthur},
	title = {A Study on the Importance of and Time Spent on Different Modeling Steps},
	year = {2012},
	issue_date = {December 2011},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {13},
	number = {2},
	issn = {1931-0145},
	url = {https://doi.org/10.1145/2207243.2207253},
	doi = {10.1145/2207243.2207253},
	abstract = {Applying data mining and machine learning algorithms requires many steps to prepare
	data and to make use of modeling results. This study investigates two questions: (1)
	how time consuming are the pre- and post-processing steps? (2) how much research energy
	is spent on these steps? To answer these questions I surveyed practitioners about
	their experiences in applying modeling techniques and categorized data mining and
	machine learning research papers from 2009 according to the modeling step(s) they
	addressed. Survey results show that model building consumes only 14% of the time spent
	on a typical project; the remaining time is spent on pre- and post-processing steps.
	Both survey responses and the categorization of research papers show that data mining
	and machine learning researchers spend the majority of their energy on algorithms
	for constructing models and significantly less energy on other steps. These findings
	collectively suggest that there are research opportunities to simplify the steps that
	precede and follow model building.},
	journal = {SIGKDD Explor. Newsl.},
	month = may,
	pages = {65–71},
	numpages = {7}
}

@article{raudys1991small,
	title={Small sample size effects in statistical pattern recognition: Recommendations for practitioners},
	author={Raudys, Sarunas J and Jain, Anil K and others},
	journal={IEEE Transactions on pattern analysis and machine intelligence},
	volume={13},
	number={3},
	pages={252--264},
	year={1991}
}

@book{swingler1996applying,
	title={Applying neural networks: a practical guide},
	author={Swingler, Kevin},
	year={1996},
	publisher={Morgan Kaufmann}
}

@book{chollet2017deep,
	title={Deep learning with Python},
	author={Chollet, Francois},
	year={2017},
	publisher={Simon and Schuster}
}

@book{geron2019hands,
	title={Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems},
	author={G{\'e}ron, Aur{\'e}lien},
	year={2019},
	publisher={O'Reilly Media}
}

@misc{wiatrak2020stabilizing,
	title={Stabilizing Generative Adversarial Networks: A Survey}, 
	author={Maciej Wiatrak and Stefano V. Albrecht and Andrew Nystrom},
	year={2020},
	eprint={1910.00927},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@book{el2020practical,
	title={Practical Synthetic Data Generation: Balancing Privacy and the Broad Availability of Data},
	author={El Emam, Khaled and Mosquera, Lucy and Hoptroff, Richard},
	year={2020},
	publisher={O'Reilly Media}
}

@article{srivastava2014dropout,
	title={Dropout: a simple way to prevent neural networks from overfitting},
	author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	journal={The journal of machine learning research},
	volume={15},
	number={1},
	pages={1929--1958},
	year={2014},
	publisher={JMLR. org}
}

@inproceedings{dietterich2000ensemble,
	title={Ensemble methods in machine learning},
	author={Dietterich, Thomas G},
	booktitle={International workshop on multiple classifier systems},
	pages={1--15},
	year={2000},
	organization={Springer}
}

@inproceedings{kfold,
	title={K-Fold Cross Validation for Error Rate Estimate in Support Vector Machines.},
	author={Anguita, Davide and Ghio, Alessandro and Ridella, Sandro and Sterpi, Dario},
	booktitle={DMIN},
	pages={291--297},
	year={2009}
}

@inproceedings{patki2016synthetic,
	title={The synthetic data vault},
	author={Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},
	booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
	pages={399--410},
	year={2016},
	organization={IEEE}
}