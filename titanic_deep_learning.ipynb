{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0079a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, Model, callbacks\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from process_data import process_data\n",
    "from nn_gridsearch import logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdfedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220abee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train\n",
      "<class 'numpy.ndarray'>\n",
      "x_train_processed\n",
      "<class 'numpy.ndarray'>\n",
      "x_test\n",
      "<class 'numpy.ndarray'>\n",
      "x_test_processed\n",
      "<class 'numpy.ndarray'>\n",
      "y_train\n",
      "<class 'numpy.ndarray'>\n",
      "y_test\n",
      "<class 'numpy.ndarray'>\n",
      "pipeline\n",
      "<class 'sklearn.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "for k, v in data.items():\n",
    "    print(k)\n",
    "    print(type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfcc0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_processed = data['x_train_processed']\n",
    "y_train = data['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "824cc18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1047, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdeb5d6",
   "metadata": {},
   "source": [
    "> note: old model constrution, using Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d1d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = models.Sequential()\n",
    "#model.add(layers.Dense(32, activation='relu', input_shape=(x_train_processed.shape[1],)))\n",
    "#model.add(layers.Dense(32, activation='relu'))\n",
    "#model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa5099",
   "metadata": {},
   "source": [
    "> changed to functional API --> more flexibility, for future expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140bb4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=x_train_processed.shape[1:])\n",
    "hidden_0 = layers.Dense(80, activation='relu')(input_)\n",
    "hidden_1 = layers.Dense(80, activation='relu')(hidden_0)\n",
    "hidden_2 = layers.Dense(80, activation='relu')(hidden_1)\n",
    "hidden_3 = layers.Dense(80, activation='relu')(hidden_2)\n",
    "hidden_4 = layers.Dense(80, activation='relu')(hidden_3)\n",
    "hidden_5 = layers.Dense(80, activation='relu')(hidden_4)\n",
    "output = layers.Dense(1, activation='sigmoid')(hidden_5)\n",
    "model_custom = Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ee6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_custom.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Precision(), Recall(), AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfb8f113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 11)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 80)                960       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 33,441\n",
      "Trainable params: 33,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_custom.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c91fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model_custom.fit(\n",
    "#    data['x_train_processed'], data['y_train'], validation_split=.2, epochs=300,\n",
    "#    callbacks=[\n",
    "#        EarlyStopping(patience=10, monitor='val_loss', mode='min'),\n",
    "#        TensorBoard(logdir('custom_model'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a7fd8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-be583b5ddf3be2b8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-be583b5ddf3be2b8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./custom_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a53ab9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gridsearch = models.load_model('./models/titanic_gridsearch_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01fc7423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_layer_0_relu_alpha_0. (None, 31)                372       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 31)                124       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 31)                0         \n",
      "_________________________________________________________________\n",
      "dropout_0_30 (Dropout)       (None, 31)                0         \n",
      "_________________________________________________________________\n",
      "hidden_layer_1_relu_alpha_0. (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 31)                124       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 31)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1_30 (Dropout)       (None, 31)                0         \n",
      "_________________________________________________________________\n",
      "hidden_layer_2_relu_alpha_0. (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 31)                124       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 31)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2_30 (Dropout)       (None, 31)                0         \n",
      "_________________________________________________________________\n",
      "hidden_layer_3_relu_alpha_0. (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 31)                124       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 31)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3_30 (Dropout)       (None, 31)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 32        \n",
      "=================================================================\n",
      "Total params: 3,876\n",
      "Trainable params: 3,628\n",
      "Non-trainable params: 248\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gridsearch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0eeea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 5ms/step - loss: 0.6955 - accuracy: 0.3626 - precision: 0.3704 - recall: 0.5932 - auc: 0.3127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6955111622810364,\n",
       " 0.36259540915489197,\n",
       " 0.37037035822868347,\n",
       " 0.5932203531265259,\n",
       " 0.3127354383468628]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_custom.evaluate(data['x_test_processed'], data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b866877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 4ms/step - loss: 0.5056 - accuracy: 0.7710 - precision_1: 0.8816 - recall_1: 0.5678 - auc_1: 0.8524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5055856704711914,\n",
       " 0.7709923386573792,\n",
       " 0.8815789222717285,\n",
       " 0.5677965879440308,\n",
       " 0.8524011373519897]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gridsearch.evaluate(data['x_test_processed'], data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd86b7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 7ms/step - loss: 0.5070 - accuracy: 0.7786 - precision_1: 0.8846 - recall_1: 0.5847 - auc_1: 0.8276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5069646239280701,\n",
       " 0.7786259651184082,\n",
       " 0.8846153616905212,\n",
       " 0.5847457647323608,\n",
       " 0.8275953531265259]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_synthetic = models.load_model('./models/titanic_gridsearch_synthetic_2021_10_15_23_42.h5')\n",
    "model_synthetic.evaluate(data['x_test_processed'], data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43e384d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 8ms/step - loss: 0.5190 - accuracy: 0.7672 - precision_1: 0.8701 - recall_1: 0.5678 - auc_1: 0.8065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5189646482467651,\n",
       " 0.767175555229187,\n",
       " 0.8701298832893372,\n",
       " 0.5677965879440308,\n",
       " 0.8064677715301514]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_synthetic_bigger = models.load_model('./models/titanic_gridsearch_synthetic_2021_10_16_05_59.h5')\n",
    "model_synthetic_bigger.evaluate(data['x_test_processed'], data['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883f560",
   "metadata": {},
   "source": [
    "> todo next - function to build model, sklearn wrapper from keras, pipelines, random grid search for model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c1d968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_layer_0_relu_alpha_0. (None, 79)                948       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 79)                0         \n",
      "_________________________________________________________________\n",
      "dropout_0_50 (Dropout)       (None, 79)                0         \n",
      "_________________________________________________________________\n",
      "hidden_layer_1_relu_alpha_0. (None, 79)                6320      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 79)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1_50 (Dropout)       (None, 79)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 80        \n",
      "=================================================================\n",
      "Total params: 7,348\n",
      "Trainable params: 7,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_synthetic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa3c8729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 6ms/step - loss: 0.5203 - accuracy: 0.7672 - precision_1: 0.8065 - recall_1: 0.6356 - auc_1: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5202765464782715,\n",
       " 0.767175555229187,\n",
       " 0.8064516186714172,\n",
       " 0.6355932354927063,\n",
       " 0.8000234961509705]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_synthetic_shrink = models.load_model('./models/titanic_gridsearch_synthetic_2021_10_16_11_17.h5')\n",
    "model_synthetic_shrink.evaluate(data['x_test_processed'], data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a336a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 11ms/step - loss: 0.5312 - accuracy: 0.7519 - precision_1: 0.8193 - recall_1: 0.5763 - auc_1: 0.7880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5311660766601562,\n",
       " 0.7519084215164185,\n",
       " 0.8192771077156067,\n",
       " 0.5762711763381958,\n",
       " 0.7879884243011475]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pure_shrink = models.load_model('models/titanic_gridsearch_2021_10_16_12_26.h5')\n",
    "model_pure_shrink.evaluate(data['x_test_processed'], data['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d370032",
   "metadata": {},
   "source": [
    "## experiments - progressively shrink data, compare model performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd82fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================\n",
      "creating data: 0.1 of data boosted: True\n",
      "boosting real data times: 3\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.01 min | Avg Losses >> G/D 0.8511/1.3935 [D-Real: 0.4722 D-Fake: 0.9212]\n",
      "Epoch 002 | ET 0.01 min | Avg Losses >> G/D 0.8634/1.1655 [D-Real: 0.3436 D-Fake: 0.8219]\n",
      "Epoch 003 | ET 0.01 min | Avg Losses >> G/D 0.7730/1.1695 [D-Real: 0.3566 D-Fake: 0.8129]\n",
      "Epoch 004 | ET 0.01 min | Avg Losses >> G/D 0.9903/0.9573 [D-Real: 0.2923 D-Fake: 0.6651]\n",
      "Epoch 005 | ET 0.02 min | Avg Losses >> G/D 0.9032/1.0286 [D-Real: 0.3108 D-Fake: 0.7178]\n",
      "Epoch 006 | ET 0.02 min | Avg Losses >> G/D 0.8396/1.0399 [D-Real: 0.3175 D-Fake: 0.7224]\n",
      "Epoch 007 | ET 0.02 min | Avg Losses >> G/D 0.9075/0.9644 [D-Real: 0.2585 D-Fake: 0.7060]\n",
      "Epoch 008 | ET 0.02 min | Avg Losses >> G/D 0.8813/0.9236 [D-Real: 0.2320 D-Fake: 0.6916]\n",
      "Epoch 009 | ET 0.02 min | Avg Losses >> G/D 0.9183/1.0039 [D-Real: 0.2377 D-Fake: 0.7662]\n",
      "Epoch 010 | ET 0.03 min | Avg Losses >> G/D 0.9173/0.9173 [D-Real: 0.2137 D-Fake: 0.7036]\n",
      "Epoch 011 | ET 0.03 min | Avg Losses >> G/D 0.9145/0.8553 [D-Real: 0.2328 D-Fake: 0.6224]\n",
      "Epoch 012 | ET 0.03 min | Avg Losses >> G/D 0.8802/0.8380 [D-Real: 0.2000 D-Fake: 0.6381]\n",
      "Epoch 013 | ET 0.03 min | Avg Losses >> G/D 0.8606/0.8928 [D-Real: 0.2267 D-Fake: 0.6660]\n",
      "Epoch 014 | ET 0.03 min | Avg Losses >> G/D 0.9526/0.8628 [D-Real: 0.2089 D-Fake: 0.6539]\n",
      "Epoch 015 | ET 0.03 min | Avg Losses >> G/D 0.8770/0.8043 [D-Real: 0.1923 D-Fake: 0.6121]\n",
      "Epoch 016 | ET 0.04 min | Avg Losses >> G/D 1.0074/0.8596 [D-Real: 0.2092 D-Fake: 0.6504]\n",
      "Epoch 017 | ET 0.04 min | Avg Losses >> G/D 0.9729/0.9227 [D-Real: 0.2217 D-Fake: 0.7010]\n",
      "Epoch 018 | ET 0.04 min | Avg Losses >> G/D 0.9741/0.8616 [D-Real: 0.2274 D-Fake: 0.6342]\n",
      "Epoch 019 | ET 0.04 min | Avg Losses >> G/D 0.9057/0.8542 [D-Real: 0.2217 D-Fake: 0.6324]\n",
      "Epoch 020 | ET 0.04 min | Avg Losses >> G/D 0.8343/0.9387 [D-Real: 0.2076 D-Fake: 0.7311]\n",
      "Epoch 021 | ET 0.04 min | Avg Losses >> G/D 0.9782/0.7781 [D-Real: 0.1654 D-Fake: 0.6126]\n",
      "Epoch 022 | ET 0.05 min | Avg Losses >> G/D 0.9986/0.8390 [D-Real: 0.2125 D-Fake: 0.6264]\n",
      "Epoch 023 | ET 0.05 min | Avg Losses >> G/D 0.8758/0.8810 [D-Real: 0.2249 D-Fake: 0.6561]\n",
      "Epoch 024 | ET 0.05 min | Avg Losses >> G/D 0.9447/0.9208 [D-Real: 0.2385 D-Fake: 0.6823]\n",
      "Epoch 025 | ET 0.05 min | Avg Losses >> G/D 1.0536/0.8277 [D-Real: 0.2237 D-Fake: 0.6040]\n",
      "Epoch 026 | ET 0.06 min | Avg Losses >> G/D 0.9464/0.8004 [D-Real: 0.1928 D-Fake: 0.6076]\n",
      "Epoch 027 | ET 0.06 min | Avg Losses >> G/D 0.9954/0.8374 [D-Real: 0.2375 D-Fake: 0.5999]\n",
      "Epoch 028 | ET 0.07 min | Avg Losses >> G/D 0.9331/0.8431 [D-Real: 0.2066 D-Fake: 0.6364]\n",
      "Epoch 029 | ET 0.07 min | Avg Losses >> G/D 0.9630/0.7939 [D-Real: 0.2234 D-Fake: 0.5705]\n",
      "Epoch 030 | ET 0.07 min | Avg Losses >> G/D 1.0221/0.8515 [D-Real: 0.2384 D-Fake: 0.6131]\n",
      "Epoch 031 | ET 0.08 min | Avg Losses >> G/D 1.0641/0.8601 [D-Real: 0.2437 D-Fake: 0.6164]\n",
      "Epoch 032 | ET 0.08 min | Avg Losses >> G/D 1.0340/0.8823 [D-Real: 0.2502 D-Fake: 0.6321]\n",
      "Epoch 033 | ET 0.08 min | Avg Losses >> G/D 1.0453/0.8271 [D-Real: 0.2857 D-Fake: 0.5414]\n",
      "Epoch 034 | ET 0.08 min | Avg Losses >> G/D 1.0631/0.8795 [D-Real: 0.3025 D-Fake: 0.5770]\n",
      "Epoch 035 | ET 0.08 min | Avg Losses >> G/D 0.9376/0.8975 [D-Real: 0.2837 D-Fake: 0.6138]\n",
      "Epoch 036 | ET 0.08 min | Avg Losses >> G/D 1.0303/0.8220 [D-Real: 0.2967 D-Fake: 0.5253]\n",
      "Epoch 037 | ET 0.09 min | Avg Losses >> G/D 0.9896/0.9932 [D-Real: 0.3910 D-Fake: 0.6022]\n",
      "Epoch 038 | ET 0.09 min | Avg Losses >> G/D 1.0240/0.8998 [D-Real: 0.3265 D-Fake: 0.5733]\n",
      "Epoch 039 | ET 0.09 min | Avg Losses >> G/D 1.0078/0.8887 [D-Real: 0.3279 D-Fake: 0.5608]\n",
      "Epoch 040 | ET 0.09 min | Avg Losses >> G/D 1.0615/0.8844 [D-Real: 0.3302 D-Fake: 0.5542]\n",
      "real\n",
      "created training data: 312 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 22ms/step - loss: 0.5566 - accuracy: 0.7634 - precision_11: 0.7642 - recall_11: 0.6864 - auc_11: 0.8281\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7633587718009949,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 3,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f74265b2eb0>,\n",
      " 'number_training_samples': 312,\n",
      " 'share_real_data': 0.1}\n",
      "\n",
      "================\n",
      "creating data: 0.1 of data boosted: False\n",
      "boosting real data times: 3\n",
      "real\n",
      "created training data: 104 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 7ms/step - loss: 0.7998 - accuracy: 0.7557 - precision_12: 0.8068 - recall_12: 0.6017 - auc_12: 0.7453\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7557252049446106,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f74e520ad30>,\n",
      " 'number_training_samples': 104,\n",
      " 'share_real_data': 0.1}\n",
      "\n",
      "================\n",
      "creating data: 0.2 of data boosted: True\n",
      "boosting real data times: 3\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.00 min | Avg Losses >> G/D 0.7402/1.4418 [D-Real: 0.6281 D-Fake: 0.8138]\n",
      "Epoch 002 | ET 0.01 min | Avg Losses >> G/D 0.8085/1.1791 [D-Real: 0.3793 D-Fake: 0.7997]\n",
      "Epoch 003 | ET 0.01 min | Avg Losses >> G/D 0.8856/1.0783 [D-Real: 0.3742 D-Fake: 0.7042]\n",
      "Epoch 004 | ET 0.01 min | Avg Losses >> G/D 0.8748/1.0554 [D-Real: 0.3296 D-Fake: 0.7259]\n",
      "Epoch 005 | ET 0.02 min | Avg Losses >> G/D 0.9609/0.9455 [D-Real: 0.2668 D-Fake: 0.6787]\n",
      "Epoch 006 | ET 0.02 min | Avg Losses >> G/D 0.9493/0.9031 [D-Real: 0.2376 D-Fake: 0.6654]\n",
      "Epoch 007 | ET 0.02 min | Avg Losses >> G/D 1.0266/0.8325 [D-Real: 0.2177 D-Fake: 0.6148]\n",
      "Epoch 008 | ET 0.02 min | Avg Losses >> G/D 0.9935/0.8626 [D-Real: 0.2195 D-Fake: 0.6431]\n",
      "Epoch 009 | ET 0.03 min | Avg Losses >> G/D 1.0721/0.7840 [D-Real: 0.2056 D-Fake: 0.5785]\n",
      "Epoch 010 | ET 0.03 min | Avg Losses >> G/D 1.0558/0.7787 [D-Real: 0.2008 D-Fake: 0.5780]\n",
      "Epoch 011 | ET 0.03 min | Avg Losses >> G/D 1.0782/0.7144 [D-Real: 0.1644 D-Fake: 0.5500]\n",
      "Epoch 012 | ET 0.04 min | Avg Losses >> G/D 1.1202/0.7182 [D-Real: 0.1617 D-Fake: 0.5565]\n",
      "Epoch 013 | ET 0.04 min | Avg Losses >> G/D 1.1584/0.7122 [D-Real: 0.1931 D-Fake: 0.5191]\n",
      "Epoch 014 | ET 0.04 min | Avg Losses >> G/D 1.2109/0.7251 [D-Real: 0.1659 D-Fake: 0.5592]\n",
      "Epoch 015 | ET 0.04 min | Avg Losses >> G/D 1.1728/0.6899 [D-Real: 0.1825 D-Fake: 0.5074]\n",
      "Epoch 016 | ET 0.05 min | Avg Losses >> G/D 1.1497/0.7530 [D-Real: 0.1713 D-Fake: 0.5817]\n",
      "Epoch 017 | ET 0.05 min | Avg Losses >> G/D 1.2847/0.6577 [D-Real: 0.1858 D-Fake: 0.4719]\n",
      "Epoch 018 | ET 0.05 min | Avg Losses >> G/D 1.2533/0.7026 [D-Real: 0.2005 D-Fake: 0.5021]\n",
      "Epoch 019 | ET 0.06 min | Avg Losses >> G/D 1.0749/0.8006 [D-Real: 0.2173 D-Fake: 0.5833]\n",
      "Epoch 020 | ET 0.06 min | Avg Losses >> G/D 1.2115/0.7396 [D-Real: 0.2267 D-Fake: 0.5129]\n",
      "Epoch 021 | ET 0.06 min | Avg Losses >> G/D 1.2561/0.8336 [D-Real: 0.2764 D-Fake: 0.5572]\n",
      "Epoch 022 | ET 0.06 min | Avg Losses >> G/D 1.3191/0.7461 [D-Real: 0.2881 D-Fake: 0.4580]\n",
      "Epoch 023 | ET 0.07 min | Avg Losses >> G/D 1.2363/0.7673 [D-Real: 0.2614 D-Fake: 0.5060]\n",
      "Epoch 024 | ET 0.07 min | Avg Losses >> G/D 1.3034/0.8112 [D-Real: 0.3008 D-Fake: 0.5104]\n",
      "Epoch 025 | ET 0.07 min | Avg Losses >> G/D 1.3833/0.8322 [D-Real: 0.3348 D-Fake: 0.4975]\n",
      "Epoch 026 | ET 0.08 min | Avg Losses >> G/D 1.2346/0.8733 [D-Real: 0.3473 D-Fake: 0.5259]\n",
      "Epoch 027 | ET 0.08 min | Avg Losses >> G/D 1.2766/0.8299 [D-Real: 0.3393 D-Fake: 0.4906]\n",
      "Epoch 028 | ET 0.08 min | Avg Losses >> G/D 1.2531/0.8600 [D-Real: 0.3595 D-Fake: 0.5005]\n",
      "Epoch 029 | ET 0.08 min | Avg Losses >> G/D 1.2220/0.9617 [D-Real: 0.3849 D-Fake: 0.5768]\n",
      "Epoch 030 | ET 0.09 min | Avg Losses >> G/D 1.3069/0.9641 [D-Real: 0.4261 D-Fake: 0.5380]\n",
      "Epoch 031 | ET 0.09 min | Avg Losses >> G/D 1.2152/1.0342 [D-Real: 0.4707 D-Fake: 0.5635]\n",
      "Epoch 032 | ET 0.09 min | Avg Losses >> G/D 1.2774/0.9552 [D-Real: 0.4335 D-Fake: 0.5217]\n",
      "Epoch 033 | ET 0.10 min | Avg Losses >> G/D 1.2304/0.9920 [D-Real: 0.4667 D-Fake: 0.5253]\n",
      "Epoch 034 | ET 0.10 min | Avg Losses >> G/D 1.1663/1.1038 [D-Real: 0.5201 D-Fake: 0.5837]\n",
      "Epoch 035 | ET 0.10 min | Avg Losses >> G/D 1.1619/1.1888 [D-Real: 0.5858 D-Fake: 0.6030]\n",
      "Epoch 036 | ET 0.10 min | Avg Losses >> G/D 1.1316/1.1302 [D-Real: 0.5495 D-Fake: 0.5808]\n",
      "Epoch 037 | ET 0.11 min | Avg Losses >> G/D 1.1532/1.1825 [D-Real: 0.6112 D-Fake: 0.5713]\n",
      "Epoch 038 | ET 0.11 min | Avg Losses >> G/D 1.1060/1.2826 [D-Real: 0.7059 D-Fake: 0.5767]\n",
      "Epoch 039 | ET 0.11 min | Avg Losses >> G/D 1.1362/1.2784 [D-Real: 0.6270 D-Fake: 0.6514]\n",
      "Epoch 040 | ET 0.12 min | Avg Losses >> G/D 1.0665/1.3730 [D-Real: 0.7351 D-Fake: 0.6379]\n",
      "real\n",
      "created training data: 627 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.5439 - accuracy: 0.7595 - precision_13: 0.7778 - recall_13: 0.6525 - auc_13: 0.8107\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7595419883728027,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 3,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f750a721d90>,\n",
      " 'number_training_samples': 627,\n",
      " 'share_real_data': 0.2}\n",
      "\n",
      "================\n",
      "creating data: 0.2 of data boosted: False\n",
      "boosting real data times: 3\n",
      "real\n",
      "created training data: 209 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 9ms/step - loss: 0.5332 - accuracy: 0.7405 - precision_14: 0.8571 - recall_14: 0.5085 - auc_14: 0.8182\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7404580116271973,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f74e4b15880>,\n",
      " 'number_training_samples': 209,\n",
      " 'share_real_data': 0.2}\n",
      "\n",
      "================\n",
      "creating data: 0.30000000000000004 of data boosted: True\n",
      "boosting real data times: 3\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.01 min | Avg Losses >> G/D 0.8113/1.4851 [D-Real: 0.7651 D-Fake: 0.7200]\n",
      "Epoch 002 | ET 0.01 min | Avg Losses >> G/D 0.7406/1.2583 [D-Real: 0.5337 D-Fake: 0.7246]\n",
      "Epoch 003 | ET 0.01 min | Avg Losses >> G/D 0.7860/1.1262 [D-Real: 0.4364 D-Fake: 0.6898]\n",
      "Epoch 004 | ET 0.02 min | Avg Losses >> G/D 0.8916/0.9926 [D-Real: 0.3663 D-Fake: 0.6262]\n",
      "Epoch 005 | ET 0.02 min | Avg Losses >> G/D 0.8792/0.9664 [D-Real: 0.2906 D-Fake: 0.6758]\n",
      "Epoch 006 | ET 0.03 min | Avg Losses >> G/D 0.9330/0.9070 [D-Real: 0.2780 D-Fake: 0.6290]\n",
      "Epoch 007 | ET 0.03 min | Avg Losses >> G/D 0.9413/0.8285 [D-Real: 0.2400 D-Fake: 0.5885]\n",
      "Epoch 008 | ET 0.03 min | Avg Losses >> G/D 0.9563/0.8000 [D-Real: 0.2285 D-Fake: 0.5714]\n",
      "Epoch 009 | ET 0.04 min | Avg Losses >> G/D 0.9442/0.8317 [D-Real: 0.2192 D-Fake: 0.6125]\n",
      "Epoch 010 | ET 0.04 min | Avg Losses >> G/D 0.9935/0.7381 [D-Real: 0.1835 D-Fake: 0.5547]\n",
      "Epoch 011 | ET 0.05 min | Avg Losses >> G/D 1.0229/0.7271 [D-Real: 0.1858 D-Fake: 0.5412]\n",
      "Epoch 012 | ET 0.05 min | Avg Losses >> G/D 1.0173/0.7832 [D-Real: 0.2206 D-Fake: 0.5626]\n",
      "Epoch 013 | ET 0.05 min | Avg Losses >> G/D 1.0375/0.7699 [D-Real: 0.2088 D-Fake: 0.5611]\n",
      "Epoch 014 | ET 0.06 min | Avg Losses >> G/D 1.1180/0.7490 [D-Real: 0.2300 D-Fake: 0.5190]\n",
      "Epoch 015 | ET 0.06 min | Avg Losses >> G/D 1.0853/0.7635 [D-Real: 0.2559 D-Fake: 0.5076]\n",
      "Epoch 016 | ET 0.06 min | Avg Losses >> G/D 1.0885/0.7786 [D-Real: 0.2792 D-Fake: 0.4994]\n",
      "Epoch 017 | ET 0.09 min | Avg Losses >> G/D 1.1144/0.8316 [D-Real: 0.2953 D-Fake: 0.5363]\n",
      "Epoch 018 | ET 0.10 min | Avg Losses >> G/D 1.0692/0.8017 [D-Real: 0.2952 D-Fake: 0.5066]\n",
      "Epoch 019 | ET 0.10 min | Avg Losses >> G/D 1.1716/0.8337 [D-Real: 0.3472 D-Fake: 0.4865]\n",
      "Epoch 020 | ET 0.11 min | Avg Losses >> G/D 1.1417/0.8912 [D-Real: 0.3713 D-Fake: 0.5199]\n",
      "Epoch 021 | ET 0.11 min | Avg Losses >> G/D 1.0962/0.9493 [D-Real: 0.4204 D-Fake: 0.5289]\n",
      "Epoch 022 | ET 0.12 min | Avg Losses >> G/D 1.0883/1.0021 [D-Real: 0.4647 D-Fake: 0.5373]\n",
      "Epoch 023 | ET 0.12 min | Avg Losses >> G/D 1.0525/1.0561 [D-Real: 0.4990 D-Fake: 0.5570]\n",
      "Epoch 024 | ET 0.13 min | Avg Losses >> G/D 1.0189/1.1341 [D-Real: 0.5413 D-Fake: 0.5928]\n",
      "Epoch 025 | ET 0.13 min | Avg Losses >> G/D 1.0062/1.1896 [D-Real: 0.5658 D-Fake: 0.6238]\n",
      "Epoch 026 | ET 0.13 min | Avg Losses >> G/D 1.0100/1.2532 [D-Real: 0.6454 D-Fake: 0.6077]\n",
      "Epoch 027 | ET 0.14 min | Avg Losses >> G/D 0.8449/1.3550 [D-Real: 0.7072 D-Fake: 0.6478]\n",
      "Epoch 028 | ET 0.14 min | Avg Losses >> G/D 0.8894/1.3148 [D-Real: 0.6735 D-Fake: 0.6414]\n",
      "Epoch 029 | ET 0.15 min | Avg Losses >> G/D 0.9091/1.4137 [D-Real: 0.7740 D-Fake: 0.6398]\n",
      "Epoch 030 | ET 0.15 min | Avg Losses >> G/D 0.8677/1.3818 [D-Real: 0.7273 D-Fake: 0.6545]\n",
      "Epoch 031 | ET 0.15 min | Avg Losses >> G/D 0.8425/1.4515 [D-Real: 0.7694 D-Fake: 0.6821]\n",
      "Epoch 032 | ET 0.16 min | Avg Losses >> G/D 0.8773/1.5190 [D-Real: 0.8185 D-Fake: 0.7005]\n",
      "Epoch 033 | ET 0.16 min | Avg Losses >> G/D 0.8708/1.4860 [D-Real: 0.8544 D-Fake: 0.6316]\n",
      "Epoch 034 | ET 0.17 min | Avg Losses >> G/D 0.8518/1.5506 [D-Real: 0.8908 D-Fake: 0.6598]\n",
      "Epoch 035 | ET 0.17 min | Avg Losses >> G/D 0.8725/1.5597 [D-Real: 0.9218 D-Fake: 0.6379]\n",
      "Epoch 036 | ET 0.17 min | Avg Losses >> G/D 0.8580/1.5513 [D-Real: 0.8958 D-Fake: 0.6554]\n",
      "Epoch 037 | ET 0.18 min | Avg Losses >> G/D 0.8609/1.5499 [D-Real: 0.9239 D-Fake: 0.6260]\n",
      "Epoch 038 | ET 0.18 min | Avg Losses >> G/D 0.8841/1.5698 [D-Real: 0.9307 D-Fake: 0.6392]\n",
      "Epoch 039 | ET 0.19 min | Avg Losses >> G/D 0.8713/1.5661 [D-Real: 0.9272 D-Fake: 0.6388]\n",
      "Epoch 040 | ET 0.19 min | Avg Losses >> G/D 0.8482/1.5513 [D-Real: 0.9167 D-Fake: 0.6346]\n",
      "real\n",
      "created training data: 942 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 18ms/step - loss: 0.5647 - accuracy: 0.7405 - precision_15: 0.7660 - recall_15: 0.6102 - auc_15: 0.7933\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7404580116271973,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 3,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73f5a59310>,\n",
      " 'number_training_samples': 942,\n",
      " 'share_real_data': 0.30000000000000004}\n",
      "\n",
      "================\n",
      "creating data: 0.30000000000000004 of data boosted: False\n",
      "boosting real data times: 3\n",
      "real\n",
      "created training data: 314 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 12ms/step - loss: 0.5044 - accuracy: 0.7672 - precision_16: 0.8701 - recall_16: 0.5678 - auc_16: 0.8561\n",
      "finished grid - results:\n",
      "{'accuracy': 0.767175555229187,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73f0078d60>,\n",
      " 'number_training_samples': 314,\n",
      " 'share_real_data': 0.30000000000000004}\n",
      "\n",
      "================\n",
      "creating data: 0.4 of data boosted: True\n",
      "boosting real data times: 3\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.01 min | Avg Losses >> G/D 0.8152/1.2538 [D-Real: 0.5077 D-Fake: 0.7461]\n",
      "Epoch 002 | ET 0.01 min | Avg Losses >> G/D 0.8301/1.0174 [D-Real: 0.2955 D-Fake: 0.7219]\n",
      "Epoch 003 | ET 0.02 min | Avg Losses >> G/D 0.9310/0.9001 [D-Real: 0.2249 D-Fake: 0.6752]\n",
      "Epoch 004 | ET 0.02 min | Avg Losses >> G/D 0.9024/0.8704 [D-Real: 0.1887 D-Fake: 0.6817]\n",
      "Epoch 005 | ET 0.03 min | Avg Losses >> G/D 0.9179/0.8246 [D-Real: 0.1725 D-Fake: 0.6521]\n",
      "Epoch 006 | ET 0.03 min | Avg Losses >> G/D 0.9766/0.7102 [D-Real: 0.1621 D-Fake: 0.5481]\n",
      "Epoch 007 | ET 0.04 min | Avg Losses >> G/D 1.0840/0.7154 [D-Real: 0.1639 D-Fake: 0.5515]\n",
      "Epoch 008 | ET 0.04 min | Avg Losses >> G/D 1.1184/0.7111 [D-Real: 0.2000 D-Fake: 0.5110]\n",
      "Epoch 009 | ET 0.05 min | Avg Losses >> G/D 1.1021/0.7649 [D-Real: 0.2428 D-Fake: 0.5221]\n",
      "Epoch 010 | ET 0.05 min | Avg Losses >> G/D 1.1615/0.7545 [D-Real: 0.2774 D-Fake: 0.4771]\n",
      "Epoch 011 | ET 0.06 min | Avg Losses >> G/D 1.1577/0.8363 [D-Real: 0.3104 D-Fake: 0.5260]\n",
      "Epoch 012 | ET 0.07 min | Avg Losses >> G/D 1.1265/0.9052 [D-Real: 0.3881 D-Fake: 0.5171]\n",
      "Epoch 013 | ET 0.07 min | Avg Losses >> G/D 1.1482/0.9299 [D-Real: 0.4013 D-Fake: 0.5286]\n",
      "Epoch 014 | ET 0.08 min | Avg Losses >> G/D 1.0651/1.0200 [D-Real: 0.4620 D-Fake: 0.5580]\n",
      "Epoch 015 | ET 0.08 min | Avg Losses >> G/D 1.0684/1.0535 [D-Real: 0.4889 D-Fake: 0.5646]\n",
      "Epoch 016 | ET 0.09 min | Avg Losses >> G/D 0.9806/1.1483 [D-Real: 0.5179 D-Fake: 0.6304]\n",
      "Epoch 017 | ET 0.09 min | Avg Losses >> G/D 0.9993/1.2174 [D-Real: 0.6044 D-Fake: 0.6130]\n",
      "Epoch 018 | ET 0.10 min | Avg Losses >> G/D 0.9313/1.3054 [D-Real: 0.6383 D-Fake: 0.6671]\n",
      "Epoch 019 | ET 0.10 min | Avg Losses >> G/D 0.9256/1.4453 [D-Real: 0.7203 D-Fake: 0.7250]\n",
      "Epoch 020 | ET 0.11 min | Avg Losses >> G/D 0.8461/1.4969 [D-Real: 0.7791 D-Fake: 0.7177]\n",
      "Epoch 021 | ET 0.11 min | Avg Losses >> G/D 0.8948/1.5084 [D-Real: 0.7878 D-Fake: 0.7206]\n",
      "Epoch 022 | ET 0.12 min | Avg Losses >> G/D 0.8445/1.5007 [D-Real: 0.8149 D-Fake: 0.6858]\n",
      "Epoch 023 | ET 0.12 min | Avg Losses >> G/D 0.8615/1.5120 [D-Real: 0.8266 D-Fake: 0.6854]\n",
      "Epoch 024 | ET 0.13 min | Avg Losses >> G/D 0.8423/1.5962 [D-Real: 0.9076 D-Fake: 0.6887]\n",
      "Epoch 025 | ET 0.15 min | Avg Losses >> G/D 0.8592/1.5805 [D-Real: 0.8988 D-Fake: 0.6817]\n",
      "Epoch 026 | ET 0.16 min | Avg Losses >> G/D 0.8673/1.6115 [D-Real: 0.9470 D-Fake: 0.6645]\n",
      "Epoch 027 | ET 0.17 min | Avg Losses >> G/D 0.8550/1.5924 [D-Real: 0.9356 D-Fake: 0.6569]\n",
      "Epoch 028 | ET 0.18 min | Avg Losses >> G/D 0.8561/1.6076 [D-Real: 0.9517 D-Fake: 0.6558]\n",
      "Epoch 029 | ET 0.19 min | Avg Losses >> G/D 0.8607/1.6232 [D-Real: 0.9601 D-Fake: 0.6631]\n",
      "Epoch 030 | ET 0.20 min | Avg Losses >> G/D 0.8531/1.5547 [D-Real: 0.9078 D-Fake: 0.6469]\n",
      "Epoch 031 | ET 0.20 min | Avg Losses >> G/D 0.8066/1.5543 [D-Real: 0.9015 D-Fake: 0.6529]\n",
      "Epoch 032 | ET 0.21 min | Avg Losses >> G/D 0.8436/1.5237 [D-Real: 0.8859 D-Fake: 0.6378]\n",
      "Epoch 033 | ET 0.22 min | Avg Losses >> G/D 0.8383/1.5567 [D-Real: 0.9096 D-Fake: 0.6472]\n",
      "Epoch 034 | ET 0.22 min | Avg Losses >> G/D 0.8090/1.5020 [D-Real: 0.8601 D-Fake: 0.6419]\n",
      "Epoch 035 | ET 0.23 min | Avg Losses >> G/D 0.8129/1.5157 [D-Real: 0.8804 D-Fake: 0.6353]\n",
      "Epoch 036 | ET 0.23 min | Avg Losses >> G/D 0.8046/1.4497 [D-Real: 0.8231 D-Fake: 0.6266]\n",
      "Epoch 037 | ET 0.24 min | Avg Losses >> G/D 0.8172/1.5060 [D-Real: 0.8500 D-Fake: 0.6560]\n",
      "Epoch 038 | ET 0.24 min | Avg Losses >> G/D 0.7913/1.4846 [D-Real: 0.8283 D-Fake: 0.6564]\n",
      "Epoch 039 | ET 0.26 min | Avg Losses >> G/D 0.8243/1.4265 [D-Real: 0.7955 D-Fake: 0.6310]\n",
      "Epoch 040 | ET 0.27 min | Avg Losses >> G/D 0.7879/1.4378 [D-Real: 0.8004 D-Fake: 0.6374]\n",
      "real\n",
      "created training data: 1254 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 24ms/step - loss: 0.5160 - accuracy: 0.7557 - precision_17: 0.8462 - recall_17: 0.5593 - auc_17: 0.8431\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7557252049446106,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 3,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73d81d3d30>,\n",
      " 'number_training_samples': 1254,\n",
      " 'share_real_data': 0.4}\n",
      "\n",
      "================\n",
      "creating data: 0.4 of data boosted: False\n",
      "boosting real data times: 3\n",
      "real\n",
      "created training data: 418 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 23ms/step - loss: 0.4892 - accuracy: 0.7710 - precision_18: 0.8452 - recall_18: 0.6017 - auc_18: 0.8501\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7709923386573792,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73d972f9a0>,\n",
      " 'number_training_samples': 418,\n",
      " 'share_real_data': 0.4}\n",
      "\n",
      "================\n",
      "creating data: 0.5 of data boosted: True\n",
      "boosting real data times: 3\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.01 min | Avg Losses >> G/D 0.7865/1.3322 [D-Real: 0.5887 D-Fake: 0.7436]\n",
      "Epoch 002 | ET 0.02 min | Avg Losses >> G/D 0.9047/0.9949 [D-Real: 0.3550 D-Fake: 0.6400]\n",
      "Epoch 003 | ET 0.02 min | Avg Losses >> G/D 1.0186/0.8117 [D-Real: 0.2473 D-Fake: 0.5644]\n",
      "Epoch 004 | ET 0.03 min | Avg Losses >> G/D 1.0942/0.7374 [D-Real: 0.2063 D-Fake: 0.5311]\n",
      "Epoch 005 | ET 0.04 min | Avg Losses >> G/D 1.1646/0.7217 [D-Real: 0.1971 D-Fake: 0.5246]\n",
      "Epoch 006 | ET 0.04 min | Avg Losses >> G/D 1.2407/0.6582 [D-Real: 0.1862 D-Fake: 0.4720]\n",
      "Epoch 007 | ET 0.05 min | Avg Losses >> G/D 1.2213/0.6729 [D-Real: 0.1951 D-Fake: 0.4778]\n",
      "Epoch 008 | ET 0.06 min | Avg Losses >> G/D 1.2970/0.7078 [D-Real: 0.2207 D-Fake: 0.4871]\n",
      "Epoch 009 | ET 0.06 min | Avg Losses >> G/D 1.2158/0.7508 [D-Real: 0.2491 D-Fake: 0.5018]\n",
      "Epoch 010 | ET 0.07 min | Avg Losses >> G/D 1.1931/0.8477 [D-Real: 0.3300 D-Fake: 0.5176]\n",
      "Epoch 011 | ET 0.08 min | Avg Losses >> G/D 1.1947/0.9153 [D-Real: 0.3585 D-Fake: 0.5568]\n",
      "Epoch 012 | ET 0.08 min | Avg Losses >> G/D 1.2263/0.9632 [D-Real: 0.4255 D-Fake: 0.5376]\n",
      "Epoch 013 | ET 0.09 min | Avg Losses >> G/D 1.0813/1.0645 [D-Real: 0.4741 D-Fake: 0.5903]\n",
      "Epoch 014 | ET 0.10 min | Avg Losses >> G/D 1.0758/1.1455 [D-Real: 0.5340 D-Fake: 0.6115]\n",
      "Epoch 015 | ET 0.10 min | Avg Losses >> G/D 1.0878/1.2144 [D-Real: 0.5901 D-Fake: 0.6243]\n",
      "Epoch 016 | ET 0.11 min | Avg Losses >> G/D 1.0565/1.2530 [D-Real: 0.6469 D-Fake: 0.6061]\n",
      "Epoch 017 | ET 0.12 min | Avg Losses >> G/D 1.0120/1.3345 [D-Real: 0.6956 D-Fake: 0.6389]\n",
      "Epoch 018 | ET 0.12 min | Avg Losses >> G/D 0.9622/1.4357 [D-Real: 0.7552 D-Fake: 0.6805]\n",
      "Epoch 019 | ET 0.13 min | Avg Losses >> G/D 0.9002/1.4594 [D-Real: 0.7908 D-Fake: 0.6686]\n",
      "Epoch 020 | ET 0.14 min | Avg Losses >> G/D 0.9047/1.5840 [D-Real: 0.8819 D-Fake: 0.7021]\n",
      "Epoch 021 | ET 0.14 min | Avg Losses >> G/D 0.8638/1.6135 [D-Real: 0.9387 D-Fake: 0.6748]\n",
      "Epoch 022 | ET 0.15 min | Avg Losses >> G/D 0.8668/1.6832 [D-Real: 0.9925 D-Fake: 0.6907]\n",
      "Epoch 023 | ET 0.16 min | Avg Losses >> G/D 0.8601/1.5957 [D-Real: 0.9444 D-Fake: 0.6513]\n",
      "Epoch 024 | ET 0.16 min | Avg Losses >> G/D 0.8958/1.6147 [D-Real: 0.9834 D-Fake: 0.6313]\n",
      "Epoch 025 | ET 0.17 min | Avg Losses >> G/D 0.8484/1.5496 [D-Real: 0.9425 D-Fake: 0.6071]\n",
      "Epoch 026 | ET 0.17 min | Avg Losses >> G/D 0.8677/1.5057 [D-Real: 0.8801 D-Fake: 0.6256]\n",
      "Epoch 027 | ET 0.20 min | Avg Losses >> G/D 0.8656/1.5255 [D-Real: 0.9031 D-Fake: 0.6224]\n",
      "Epoch 028 | ET 0.22 min | Avg Losses >> G/D 0.8491/1.4841 [D-Real: 0.8535 D-Fake: 0.6306]\n",
      "Epoch 029 | ET 0.23 min | Avg Losses >> G/D 0.8498/1.4630 [D-Real: 0.8596 D-Fake: 0.6033]\n",
      "Epoch 030 | ET 0.24 min | Avg Losses >> G/D 0.8358/1.4722 [D-Real: 0.8359 D-Fake: 0.6363]\n",
      "Epoch 031 | ET 0.25 min | Avg Losses >> G/D 0.8035/1.4604 [D-Real: 0.8204 D-Fake: 0.6400]\n",
      "Epoch 032 | ET 0.25 min | Avg Losses >> G/D 0.7950/1.5095 [D-Real: 0.8263 D-Fake: 0.6832]\n",
      "Epoch 033 | ET 0.26 min | Avg Losses >> G/D 0.7891/1.4283 [D-Real: 0.7631 D-Fake: 0.6652]\n",
      "Epoch 034 | ET 0.26 min | Avg Losses >> G/D 0.7985/1.4181 [D-Real: 0.7455 D-Fake: 0.6726]\n",
      "Epoch 035 | ET 0.27 min | Avg Losses >> G/D 0.8001/1.3900 [D-Real: 0.7404 D-Fake: 0.6496]\n",
      "Epoch 036 | ET 0.28 min | Avg Losses >> G/D 0.8124/1.3819 [D-Real: 0.7286 D-Fake: 0.6533]\n",
      "Epoch 037 | ET 0.28 min | Avg Losses >> G/D 0.8314/1.4022 [D-Real: 0.7482 D-Fake: 0.6540]\n",
      "Epoch 038 | ET 0.29 min | Avg Losses >> G/D 0.7563/1.3845 [D-Real: 0.7250 D-Fake: 0.6595]\n",
      "Epoch 039 | ET 0.30 min | Avg Losses >> G/D 0.7534/1.4024 [D-Real: 0.7146 D-Fake: 0.6879]\n",
      "Epoch 040 | ET 0.30 min | Avg Losses >> G/D 0.7184/1.3870 [D-Real: 0.6834 D-Fake: 0.7036]\n",
      "real\n",
      "created training data: 1569 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 28ms/step - loss: 0.5418 - accuracy: 0.7557 - precision_19: 0.8750 - recall_19: 0.5339 - auc_19: 0.7720\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7557252049446106,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 3,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73d972f0d0>,\n",
      " 'number_training_samples': 1569,\n",
      " 'share_real_data': 0.5}\n",
      "\n",
      "================\n",
      "creating data: 0.5 of data boosted: False\n",
      "boosting real data times: 3\n",
      "real\n",
      "created training data: 523 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 18ms/step - loss: 0.5307 - accuracy: 0.7405 - precision_20: 0.9167 - recall_20: 0.4661 - auc_20: 0.8309\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7404580116271973,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73d28ac100>,\n",
      " 'number_training_samples': 523,\n",
      " 'share_real_data': 0.5}\n",
      "\n",
      "================\n",
      "creating data: 0.1 of data boosted: True\n",
      "boosting real data times: 4\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.00 min | Avg Losses >> G/D 0.7584/1.6330 [D-Real: 0.9088 D-Fake: 0.7242]\n",
      "Epoch 002 | ET 0.00 min | Avg Losses >> G/D 0.7594/1.3895 [D-Real: 0.6418 D-Fake: 0.7477]\n",
      "Epoch 003 | ET 0.01 min | Avg Losses >> G/D 0.7978/1.3464 [D-Real: 0.5596 D-Fake: 0.7868]\n",
      "Epoch 004 | ET 0.01 min | Avg Losses >> G/D 0.7632/1.2867 [D-Real: 0.4917 D-Fake: 0.7949]\n",
      "Epoch 005 | ET 0.01 min | Avg Losses >> G/D 0.8300/1.2108 [D-Real: 0.4363 D-Fake: 0.7746]\n",
      "Epoch 006 | ET 0.01 min | Avg Losses >> G/D 0.8517/1.0825 [D-Real: 0.4111 D-Fake: 0.6713]\n",
      "Epoch 007 | ET 0.01 min | Avg Losses >> G/D 0.8821/1.0620 [D-Real: 0.3547 D-Fake: 0.7073]\n",
      "Epoch 008 | ET 0.02 min | Avg Losses >> G/D 0.8736/0.9916 [D-Real: 0.3474 D-Fake: 0.6442]\n",
      "Epoch 009 | ET 0.02 min | Avg Losses >> G/D 0.8670/1.0546 [D-Real: 0.3446 D-Fake: 0.7100]\n",
      "Epoch 010 | ET 0.02 min | Avg Losses >> G/D 0.8194/1.0273 [D-Real: 0.2778 D-Fake: 0.7494]\n",
      "Epoch 011 | ET 0.02 min | Avg Losses >> G/D 0.9629/0.9567 [D-Real: 0.3189 D-Fake: 0.6378]\n",
      "Epoch 012 | ET 0.02 min | Avg Losses >> G/D 0.9073/0.9962 [D-Real: 0.2658 D-Fake: 0.7304]\n",
      "Epoch 013 | ET 0.03 min | Avg Losses >> G/D 0.9896/0.9199 [D-Real: 0.2774 D-Fake: 0.6425]\n",
      "Epoch 014 | ET 0.03 min | Avg Losses >> G/D 0.9748/0.8796 [D-Real: 0.2470 D-Fake: 0.6326]\n",
      "Epoch 015 | ET 0.03 min | Avg Losses >> G/D 1.0149/0.8760 [D-Real: 0.2402 D-Fake: 0.6358]\n",
      "Epoch 016 | ET 0.03 min | Avg Losses >> G/D 0.9271/0.8724 [D-Real: 0.2582 D-Fake: 0.6142]\n",
      "Epoch 017 | ET 0.03 min | Avg Losses >> G/D 1.0962/0.8189 [D-Real: 0.2204 D-Fake: 0.5985]\n",
      "Epoch 018 | ET 0.03 min | Avg Losses >> G/D 0.9394/0.8742 [D-Real: 0.2340 D-Fake: 0.6402]\n",
      "Epoch 019 | ET 0.04 min | Avg Losses >> G/D 0.8202/0.8471 [D-Real: 0.2011 D-Fake: 0.6460]\n",
      "Epoch 020 | ET 0.04 min | Avg Losses >> G/D 0.8052/0.9416 [D-Real: 0.2236 D-Fake: 0.7180]\n",
      "Epoch 021 | ET 0.04 min | Avg Losses >> G/D 0.9764/0.7847 [D-Real: 0.1833 D-Fake: 0.6014]\n",
      "Epoch 022 | ET 0.04 min | Avg Losses >> G/D 0.9072/0.9015 [D-Real: 0.2066 D-Fake: 0.6949]\n",
      "Epoch 023 | ET 0.04 min | Avg Losses >> G/D 0.8865/0.8303 [D-Real: 0.2245 D-Fake: 0.6059]\n",
      "Epoch 024 | ET 0.05 min | Avg Losses >> G/D 0.9753/0.7589 [D-Real: 0.1890 D-Fake: 0.5699]\n",
      "Epoch 025 | ET 0.05 min | Avg Losses >> G/D 0.9939/0.8242 [D-Real: 0.2161 D-Fake: 0.6081]\n",
      "Epoch 026 | ET 0.05 min | Avg Losses >> G/D 0.9823/0.9157 [D-Real: 0.2043 D-Fake: 0.7114]\n",
      "Epoch 027 | ET 0.05 min | Avg Losses >> G/D 1.1550/0.7147 [D-Real: 0.1951 D-Fake: 0.5196]\n",
      "Epoch 028 | ET 0.05 min | Avg Losses >> G/D 0.9673/0.8172 [D-Real: 0.2159 D-Fake: 0.6013]\n",
      "Epoch 029 | ET 0.05 min | Avg Losses >> G/D 1.1402/0.7303 [D-Real: 0.2235 D-Fake: 0.5068]\n",
      "Epoch 030 | ET 0.06 min | Avg Losses >> G/D 0.9980/0.8774 [D-Real: 0.2555 D-Fake: 0.6220]\n",
      "Epoch 031 | ET 0.06 min | Avg Losses >> G/D 0.9690/0.8651 [D-Real: 0.2376 D-Fake: 0.6274]\n",
      "Epoch 032 | ET 0.06 min | Avg Losses >> G/D 1.1216/0.8279 [D-Real: 0.2506 D-Fake: 0.5772]\n",
      "Epoch 033 | ET 0.06 min | Avg Losses >> G/D 1.0326/0.8681 [D-Real: 0.3016 D-Fake: 0.5665]\n",
      "Epoch 034 | ET 0.06 min | Avg Losses >> G/D 1.0021/0.8340 [D-Real: 0.2595 D-Fake: 0.5745]\n",
      "Epoch 035 | ET 0.06 min | Avg Losses >> G/D 1.0774/0.7950 [D-Real: 0.2297 D-Fake: 0.5653]\n",
      "Epoch 036 | ET 0.07 min | Avg Losses >> G/D 1.1171/0.7950 [D-Real: 0.2825 D-Fake: 0.5125]\n",
      "Epoch 037 | ET 0.07 min | Avg Losses >> G/D 1.1788/0.8509 [D-Real: 0.3207 D-Fake: 0.5302]\n",
      "Epoch 038 | ET 0.07 min | Avg Losses >> G/D 1.0691/0.8401 [D-Real: 0.2978 D-Fake: 0.5423]\n",
      "Epoch 039 | ET 0.07 min | Avg Losses >> G/D 1.0895/0.8167 [D-Real: 0.2674 D-Fake: 0.5493]\n",
      "Epoch 040 | ET 0.07 min | Avg Losses >> G/D 1.1297/0.8182 [D-Real: 0.3192 D-Fake: 0.4989]\n",
      "real\n",
      "created training data: 416 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 23ms/step - loss: 0.6348 - accuracy: 0.7786 - precision_21: 0.7778 - recall_21: 0.7119 - auc_21: 0.8202\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7786259651184082,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 4,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73cf020130>,\n",
      " 'number_training_samples': 416,\n",
      " 'share_real_data': 0.1}\n",
      "\n",
      "================\n",
      "creating data: 0.1 of data boosted: False\n",
      "boosting real data times: 4\n",
      "real\n",
      "created training data: 104 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.6158 - accuracy: 0.7595 - precision_22: 0.8161 - recall_22: 0.6017 - auc_22: 0.8040\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7595419883728027,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73c5b63eb0>,\n",
      " 'number_training_samples': 104,\n",
      " 'share_real_data': 0.1}\n",
      "\n",
      "================\n",
      "creating data: 0.2 of data boosted: True\n",
      "boosting real data times: 4\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.00 min | Avg Losses >> G/D 0.7372/1.9479 [D-Real: 1.0988 D-Fake: 0.8491]\n",
      "Epoch 002 | ET 0.01 min | Avg Losses >> G/D 0.7428/1.5356 [D-Real: 0.7704 D-Fake: 0.7653]\n",
      "Epoch 003 | ET 0.01 min | Avg Losses >> G/D 0.7452/1.3709 [D-Real: 0.5830 D-Fake: 0.7879]\n",
      "Epoch 004 | ET 0.02 min | Avg Losses >> G/D 0.7547/1.2665 [D-Real: 0.4948 D-Fake: 0.7717]\n",
      "Epoch 005 | ET 0.04 min | Avg Losses >> G/D 0.7750/1.1242 [D-Real: 0.4289 D-Fake: 0.6953]\n",
      "Epoch 006 | ET 0.05 min | Avg Losses >> G/D 0.7936/1.0552 [D-Real: 0.3602 D-Fake: 0.6950]\n",
      "Epoch 007 | ET 0.06 min | Avg Losses >> G/D 0.8777/1.0137 [D-Real: 0.3168 D-Fake: 0.6969]\n",
      "Epoch 008 | ET 0.07 min | Avg Losses >> G/D 0.8871/0.9392 [D-Real: 0.2951 D-Fake: 0.6441]\n",
      "Epoch 009 | ET 0.08 min | Avg Losses >> G/D 0.9487/0.8773 [D-Real: 0.2694 D-Fake: 0.6080]\n",
      "Epoch 010 | ET 0.08 min | Avg Losses >> G/D 0.9344/0.8858 [D-Real: 0.2267 D-Fake: 0.6590]\n",
      "Epoch 011 | ET 0.08 min | Avg Losses >> G/D 0.9492/0.8315 [D-Real: 0.2314 D-Fake: 0.6000]\n",
      "Epoch 012 | ET 0.09 min | Avg Losses >> G/D 0.9943/0.7939 [D-Real: 0.1971 D-Fake: 0.5969]\n",
      "Epoch 013 | ET 0.09 min | Avg Losses >> G/D 0.9474/0.8144 [D-Real: 0.1945 D-Fake: 0.6199]\n",
      "Epoch 014 | ET 0.09 min | Avg Losses >> G/D 1.0304/0.7990 [D-Real: 0.1729 D-Fake: 0.6261]\n",
      "Epoch 015 | ET 0.10 min | Avg Losses >> G/D 0.9841/0.7439 [D-Real: 0.1888 D-Fake: 0.5551]\n",
      "Epoch 016 | ET 0.10 min | Avg Losses >> G/D 1.0540/0.6945 [D-Real: 0.1869 D-Fake: 0.5075]\n",
      "Epoch 017 | ET 0.10 min | Avg Losses >> G/D 1.0742/0.7256 [D-Real: 0.1749 D-Fake: 0.5507]\n",
      "Epoch 018 | ET 0.11 min | Avg Losses >> G/D 1.0471/0.7160 [D-Real: 0.1785 D-Fake: 0.5376]\n",
      "Epoch 019 | ET 0.11 min | Avg Losses >> G/D 1.0889/0.7671 [D-Real: 0.1770 D-Fake: 0.5900]\n",
      "Epoch 020 | ET 0.11 min | Avg Losses >> G/D 1.1250/0.7459 [D-Real: 0.1970 D-Fake: 0.5488]\n",
      "Epoch 021 | ET 0.12 min | Avg Losses >> G/D 1.0286/0.7719 [D-Real: 0.2155 D-Fake: 0.5564]\n",
      "Epoch 022 | ET 0.12 min | Avg Losses >> G/D 1.0331/0.8087 [D-Real: 0.1959 D-Fake: 0.6129]\n",
      "Epoch 023 | ET 0.12 min | Avg Losses >> G/D 1.1069/0.6611 [D-Real: 0.2044 D-Fake: 0.4568]\n",
      "Epoch 024 | ET 0.12 min | Avg Losses >> G/D 1.1140/0.7860 [D-Real: 0.2457 D-Fake: 0.5402]\n",
      "Epoch 025 | ET 0.13 min | Avg Losses >> G/D 1.0662/0.7652 [D-Real: 0.2404 D-Fake: 0.5248]\n",
      "Epoch 026 | ET 0.13 min | Avg Losses >> G/D 1.1101/0.7722 [D-Real: 0.2409 D-Fake: 0.5313]\n",
      "Epoch 027 | ET 0.13 min | Avg Losses >> G/D 1.2008/0.8307 [D-Real: 0.2872 D-Fake: 0.5435]\n",
      "Epoch 028 | ET 0.14 min | Avg Losses >> G/D 1.1002/0.8293 [D-Real: 0.2824 D-Fake: 0.5469]\n",
      "Epoch 029 | ET 0.14 min | Avg Losses >> G/D 1.2268/0.8371 [D-Real: 0.3203 D-Fake: 0.5168]\n",
      "Epoch 030 | ET 0.14 min | Avg Losses >> G/D 1.2181/0.7880 [D-Real: 0.2958 D-Fake: 0.4922]\n",
      "Epoch 031 | ET 0.14 min | Avg Losses >> G/D 1.1528/0.8561 [D-Real: 0.3276 D-Fake: 0.5284]\n",
      "Epoch 032 | ET 0.15 min | Avg Losses >> G/D 1.1513/0.8855 [D-Real: 0.3251 D-Fake: 0.5603]\n",
      "Epoch 033 | ET 0.15 min | Avg Losses >> G/D 1.1250/0.9804 [D-Real: 0.3950 D-Fake: 0.5853]\n",
      "Epoch 034 | ET 0.15 min | Avg Losses >> G/D 1.1243/0.9917 [D-Real: 0.4056 D-Fake: 0.5860]\n",
      "Epoch 035 | ET 0.15 min | Avg Losses >> G/D 1.1152/1.0134 [D-Real: 0.4142 D-Fake: 0.5993]\n",
      "Epoch 036 | ET 0.16 min | Avg Losses >> G/D 1.1807/1.0103 [D-Real: 0.4356 D-Fake: 0.5747]\n",
      "Epoch 037 | ET 0.16 min | Avg Losses >> G/D 1.0419/1.0776 [D-Real: 0.4915 D-Fake: 0.5862]\n",
      "Epoch 038 | ET 0.16 min | Avg Losses >> G/D 1.0874/1.0196 [D-Real: 0.4380 D-Fake: 0.5817]\n",
      "Epoch 039 | ET 0.17 min | Avg Losses >> G/D 1.0852/1.0814 [D-Real: 0.5003 D-Fake: 0.5811]\n",
      "Epoch 040 | ET 0.17 min | Avg Losses >> G/D 1.1560/1.0978 [D-Real: 0.5646 D-Fake: 0.5331]\n",
      "real\n",
      "created training data: 836 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 15ms/step - loss: 0.5945 - accuracy: 0.7405 - precision_23: 0.7717 - recall_23: 0.6017 - auc_23: 0.8047\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7404580116271973,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 4,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73c985e130>,\n",
      " 'number_training_samples': 836,\n",
      " 'share_real_data': 0.2}\n",
      "\n",
      "================\n",
      "creating data: 0.2 of data boosted: False\n",
      "boosting real data times: 4\n",
      "real\n",
      "created training data: 209 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.5514 - accuracy: 0.7405 - precision_24: 0.8205 - recall_24: 0.5424 - auc_24: 0.8101\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7404580116271973,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73d91147f0>,\n",
      " 'number_training_samples': 209,\n",
      " 'share_real_data': 0.2}\n",
      "\n",
      "================\n",
      "creating data: 0.30000000000000004 of data boosted: True\n",
      "boosting real data times: 4\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.00 min | Avg Losses >> G/D 0.7408/1.3136 [D-Real: 0.4216 D-Fake: 0.8920]\n",
      "Epoch 002 | ET 0.01 min | Avg Losses >> G/D 0.7631/1.1072 [D-Real: 0.3192 D-Fake: 0.7880]\n",
      "Epoch 003 | ET 0.01 min | Avg Losses >> G/D 0.7757/0.9920 [D-Real: 0.2667 D-Fake: 0.7253]\n",
      "Epoch 004 | ET 0.02 min | Avg Losses >> G/D 0.9204/0.8942 [D-Real: 0.2338 D-Fake: 0.6604]\n",
      "Epoch 005 | ET 0.02 min | Avg Losses >> G/D 0.9187/0.8566 [D-Real: 0.1944 D-Fake: 0.6622]\n",
      "Epoch 006 | ET 0.03 min | Avg Losses >> G/D 0.9334/0.7875 [D-Real: 0.1616 D-Fake: 0.6258]\n",
      "Epoch 007 | ET 0.03 min | Avg Losses >> G/D 1.0135/0.7628 [D-Real: 0.1554 D-Fake: 0.6074]\n",
      "Epoch 008 | ET 0.03 min | Avg Losses >> G/D 1.0618/0.7668 [D-Real: 0.1801 D-Fake: 0.5866]\n",
      "Epoch 009 | ET 0.04 min | Avg Losses >> G/D 1.0704/0.7212 [D-Real: 0.1676 D-Fake: 0.5536]\n",
      "Epoch 010 | ET 0.04 min | Avg Losses >> G/D 1.1422/0.7537 [D-Real: 0.1649 D-Fake: 0.5888]\n",
      "Epoch 011 | ET 0.05 min | Avg Losses >> G/D 1.1543/0.6857 [D-Real: 0.1786 D-Fake: 0.5071]\n",
      "Epoch 012 | ET 0.05 min | Avg Losses >> G/D 1.1650/0.6747 [D-Real: 0.1941 D-Fake: 0.4806]\n",
      "Epoch 013 | ET 0.06 min | Avg Losses >> G/D 1.1951/0.7056 [D-Real: 0.2037 D-Fake: 0.5019]\n",
      "Epoch 014 | ET 0.06 min | Avg Losses >> G/D 1.1939/0.7629 [D-Real: 0.2299 D-Fake: 0.5330]\n",
      "Epoch 015 | ET 0.06 min | Avg Losses >> G/D 1.1893/0.7540 [D-Real: 0.2399 D-Fake: 0.5141]\n",
      "Epoch 016 | ET 0.07 min | Avg Losses >> G/D 1.1398/0.7860 [D-Real: 0.2562 D-Fake: 0.5297]\n",
      "Epoch 017 | ET 0.07 min | Avg Losses >> G/D 1.1996/0.8585 [D-Real: 0.3174 D-Fake: 0.5411]\n",
      "Epoch 018 | ET 0.08 min | Avg Losses >> G/D 1.1635/0.8562 [D-Real: 0.3122 D-Fake: 0.5440]\n",
      "Epoch 019 | ET 0.09 min | Avg Losses >> G/D 1.1180/0.9259 [D-Real: 0.3664 D-Fake: 0.5595]\n",
      "Epoch 020 | ET 0.11 min | Avg Losses >> G/D 1.1451/0.9809 [D-Real: 0.4166 D-Fake: 0.5643]\n",
      "Epoch 021 | ET 0.12 min | Avg Losses >> G/D 1.1654/1.0118 [D-Real: 0.4654 D-Fake: 0.5464]\n",
      "Epoch 022 | ET 0.13 min | Avg Losses >> G/D 1.0645/1.0650 [D-Real: 0.4590 D-Fake: 0.6060]\n",
      "Epoch 023 | ET 0.14 min | Avg Losses >> G/D 1.0830/1.1861 [D-Real: 0.5633 D-Fake: 0.6228]\n",
      "Epoch 024 | ET 0.14 min | Avg Losses >> G/D 1.1989/1.1964 [D-Real: 0.5771 D-Fake: 0.6193]\n",
      "Epoch 025 | ET 0.15 min | Avg Losses >> G/D 1.0177/1.2504 [D-Real: 0.6026 D-Fake: 0.6478]\n",
      "Epoch 026 | ET 0.15 min | Avg Losses >> G/D 1.0179/1.3153 [D-Real: 0.6874 D-Fake: 0.6279]\n",
      "Epoch 027 | ET 0.16 min | Avg Losses >> G/D 0.9440/1.3786 [D-Real: 0.7071 D-Fake: 0.6715]\n",
      "Epoch 028 | ET 0.16 min | Avg Losses >> G/D 0.9966/1.4425 [D-Real: 0.7804 D-Fake: 0.6621]\n",
      "Epoch 029 | ET 0.17 min | Avg Losses >> G/D 0.9204/1.4712 [D-Real: 0.8087 D-Fake: 0.6625]\n",
      "Epoch 030 | ET 0.17 min | Avg Losses >> G/D 0.8902/1.5580 [D-Real: 0.8539 D-Fake: 0.7040]\n",
      "Epoch 031 | ET 0.18 min | Avg Losses >> G/D 0.9672/1.5772 [D-Real: 0.8680 D-Fake: 0.7092]\n",
      "Epoch 032 | ET 0.18 min | Avg Losses >> G/D 0.8934/1.6018 [D-Real: 0.9171 D-Fake: 0.6847]\n",
      "Epoch 033 | ET 0.19 min | Avg Losses >> G/D 0.8897/1.6008 [D-Real: 0.9282 D-Fake: 0.6726]\n",
      "Epoch 034 | ET 0.19 min | Avg Losses >> G/D 0.8900/1.6552 [D-Real: 0.9540 D-Fake: 0.7012]\n",
      "Epoch 035 | ET 0.20 min | Avg Losses >> G/D 0.8910/1.6054 [D-Real: 0.9340 D-Fake: 0.6714]\n",
      "Epoch 036 | ET 0.20 min | Avg Losses >> G/D 0.8339/1.6601 [D-Real: 0.9813 D-Fake: 0.6788]\n",
      "Epoch 037 | ET 0.20 min | Avg Losses >> G/D 0.8761/1.5893 [D-Real: 0.9324 D-Fake: 0.6568]\n",
      "Epoch 038 | ET 0.21 min | Avg Losses >> G/D 0.8933/1.6360 [D-Real: 0.9796 D-Fake: 0.6564]\n",
      "Epoch 039 | ET 0.21 min | Avg Losses >> G/D 0.8590/1.5579 [D-Real: 0.9397 D-Fake: 0.6183]\n",
      "Epoch 040 | ET 0.22 min | Avg Losses >> G/D 0.8303/1.6128 [D-Real: 0.9749 D-Fake: 0.6379]\n",
      "real\n",
      "created training data: 1256 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 14ms/step - loss: 0.5632 - accuracy: 0.7443 - precision_25: 0.7476 - recall_25: 0.6525 - auc_25: 0.7829\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7442747950553894,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 4,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73c7f99d60>,\n",
      " 'number_training_samples': 1256,\n",
      " 'share_real_data': 0.30000000000000004}\n",
      "\n",
      "================\n",
      "creating data: 0.30000000000000004 of data boosted: False\n",
      "boosting real data times: 4\n",
      "real\n",
      "created training data: 314 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 10ms/step - loss: 0.5189 - accuracy: 0.7710 - precision_26: 0.7843 - recall_26: 0.6780 - auc_26: 0.8419\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7709923386573792,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73c7fe14c0>,\n",
      " 'number_training_samples': 314,\n",
      " 'share_real_data': 0.30000000000000004}\n",
      "\n",
      "================\n",
      "creating data: 0.4 of data boosted: True\n",
      "boosting real data times: 4\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.01 min | Avg Losses >> G/D 0.7866/1.4023 [D-Real: 0.4660 D-Fake: 0.9363]\n",
      "Epoch 002 | ET 0.01 min | Avg Losses >> G/D 0.8764/1.0555 [D-Real: 0.2849 D-Fake: 0.7706]\n",
      "Epoch 003 | ET 0.02 min | Avg Losses >> G/D 0.9405/0.9044 [D-Real: 0.2348 D-Fake: 0.6697]\n",
      "Epoch 004 | ET 0.02 min | Avg Losses >> G/D 0.9789/0.8512 [D-Real: 0.1882 D-Fake: 0.6630]\n",
      "Epoch 005 | ET 0.03 min | Avg Losses >> G/D 1.1343/0.7653 [D-Real: 0.1759 D-Fake: 0.5894]\n",
      "Epoch 006 | ET 0.03 min | Avg Losses >> G/D 1.1990/0.7457 [D-Real: 0.1775 D-Fake: 0.5681]\n",
      "Epoch 007 | ET 0.04 min | Avg Losses >> G/D 1.2059/0.6934 [D-Real: 0.1706 D-Fake: 0.5228]\n",
      "Epoch 008 | ET 0.05 min | Avg Losses >> G/D 1.1878/0.6976 [D-Real: 0.1774 D-Fake: 0.5202]\n",
      "Epoch 009 | ET 0.05 min | Avg Losses >> G/D 1.1794/0.7549 [D-Real: 0.2079 D-Fake: 0.5471]\n",
      "Epoch 010 | ET 0.08 min | Avg Losses >> G/D 1.1670/0.7960 [D-Real: 0.2437 D-Fake: 0.5523]\n",
      "Epoch 011 | ET 0.09 min | Avg Losses >> G/D 1.1706/0.7843 [D-Real: 0.2785 D-Fake: 0.5059]\n",
      "Epoch 012 | ET 0.11 min | Avg Losses >> G/D 1.1322/0.9240 [D-Real: 0.3416 D-Fake: 0.5825]\n",
      "Epoch 013 | ET 0.12 min | Avg Losses >> G/D 1.1302/0.9275 [D-Real: 0.3852 D-Fake: 0.5423]\n",
      "Epoch 014 | ET 0.13 min | Avg Losses >> G/D 1.1249/0.9909 [D-Real: 0.4314 D-Fake: 0.5595]\n",
      "Epoch 015 | ET 0.14 min | Avg Losses >> G/D 1.1322/1.0304 [D-Real: 0.4650 D-Fake: 0.5653]\n",
      "Epoch 016 | ET 0.14 min | Avg Losses >> G/D 1.1222/1.0731 [D-Real: 0.4999 D-Fake: 0.5732]\n",
      "Epoch 017 | ET 0.15 min | Avg Losses >> G/D 1.1089/1.1078 [D-Real: 0.5404 D-Fake: 0.5674]\n",
      "Epoch 018 | ET 0.15 min | Avg Losses >> G/D 1.0860/1.1457 [D-Real: 0.5693 D-Fake: 0.5765]\n",
      "Epoch 019 | ET 0.16 min | Avg Losses >> G/D 1.0026/1.2884 [D-Real: 0.6363 D-Fake: 0.6520]\n",
      "Epoch 020 | ET 0.17 min | Avg Losses >> G/D 0.9944/1.4535 [D-Real: 0.7618 D-Fake: 0.6916]\n",
      "Epoch 021 | ET 0.17 min | Avg Losses >> G/D 0.9839/1.4632 [D-Real: 0.7796 D-Fake: 0.6836]\n",
      "Epoch 022 | ET 0.18 min | Avg Losses >> G/D 0.7881/1.5381 [D-Real: 0.7988 D-Fake: 0.7393]\n",
      "Epoch 023 | ET 0.18 min | Avg Losses >> G/D 0.8831/1.6044 [D-Real: 0.9018 D-Fake: 0.7026]\n",
      "Epoch 024 | ET 0.19 min | Avg Losses >> G/D 0.8547/1.6292 [D-Real: 0.9116 D-Fake: 0.7176]\n",
      "Epoch 025 | ET 0.19 min | Avg Losses >> G/D 0.8744/1.6258 [D-Real: 0.9125 D-Fake: 0.7133]\n",
      "Epoch 026 | ET 0.20 min | Avg Losses >> G/D 0.9172/1.5624 [D-Real: 0.9207 D-Fake: 0.6418]\n",
      "Epoch 027 | ET 0.20 min | Avg Losses >> G/D 0.8883/1.5330 [D-Real: 0.8834 D-Fake: 0.6497]\n",
      "Epoch 028 | ET 0.21 min | Avg Losses >> G/D 0.8829/1.5447 [D-Real: 0.8989 D-Fake: 0.6457]\n",
      "Epoch 029 | ET 0.22 min | Avg Losses >> G/D 0.8602/1.5645 [D-Real: 0.9028 D-Fake: 0.6617]\n",
      "Epoch 030 | ET 0.22 min | Avg Losses >> G/D 0.8680/1.5233 [D-Real: 0.8846 D-Fake: 0.6387]\n",
      "Epoch 031 | ET 0.23 min | Avg Losses >> G/D 0.8315/1.5764 [D-Real: 0.8944 D-Fake: 0.6820]\n",
      "Epoch 032 | ET 0.23 min | Avg Losses >> G/D 0.8486/1.5037 [D-Real: 0.8543 D-Fake: 0.6494]\n",
      "Epoch 033 | ET 0.24 min | Avg Losses >> G/D 0.8886/1.4924 [D-Real: 0.8740 D-Fake: 0.6185]\n",
      "Epoch 034 | ET 0.24 min | Avg Losses >> G/D 0.8153/1.4824 [D-Real: 0.8522 D-Fake: 0.6302]\n",
      "Epoch 035 | ET 0.25 min | Avg Losses >> G/D 0.8170/1.4942 [D-Real: 0.8251 D-Fake: 0.6690]\n",
      "Epoch 036 | ET 0.25 min | Avg Losses >> G/D 0.8549/1.4618 [D-Real: 0.8305 D-Fake: 0.6313]\n",
      "Epoch 037 | ET 0.26 min | Avg Losses >> G/D 0.8074/1.4357 [D-Real: 0.8124 D-Fake: 0.6233]\n",
      "Epoch 038 | ET 0.26 min | Avg Losses >> G/D 0.8340/1.4249 [D-Real: 0.7806 D-Fake: 0.6443]\n",
      "Epoch 039 | ET 0.27 min | Avg Losses >> G/D 0.8409/1.4157 [D-Real: 0.7637 D-Fake: 0.6520]\n",
      "Epoch 040 | ET 0.28 min | Avg Losses >> G/D 0.8415/1.4176 [D-Real: 0.7725 D-Fake: 0.6451]\n",
      "real\n",
      "created training data: 1672 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 22ms/step - loss: 0.5436 - accuracy: 0.7672 - precision_27: 0.8202 - recall_27: 0.6186 - auc_27: 0.8119\n",
      "finished grid - results:\n",
      "{'accuracy': 0.767175555229187,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 4,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73bc187ca0>,\n",
      " 'number_training_samples': 1672,\n",
      " 'share_real_data': 0.4}\n",
      "\n",
      "================\n",
      "creating data: 0.4 of data boosted: False\n",
      "boosting real data times: 4\n",
      "real\n",
      "created training data: 418 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 2s 24ms/step - loss: 0.5425 - accuracy: 0.7557 - precision_28: 0.8750 - recall_28: 0.5339 - auc_28: 0.7952\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7557252049446106,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73bbe1b310>,\n",
      " 'number_training_samples': 418,\n",
      " 'share_real_data': 0.4}\n",
      "\n",
      "================\n",
      "creating data: 0.5 of data boosted: True\n",
      "boosting real data times: 4\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.01 min | Avg Losses >> G/D 0.8324/1.2190 [D-Real: 0.4651 D-Fake: 0.7539]\n",
      "Epoch 002 | ET 0.02 min | Avg Losses >> G/D 0.8611/0.9271 [D-Real: 0.2715 D-Fake: 0.6556]\n",
      "Epoch 003 | ET 0.03 min | Avg Losses >> G/D 0.9082/0.8429 [D-Real: 0.2051 D-Fake: 0.6378]\n",
      "Epoch 004 | ET 0.03 min | Avg Losses >> G/D 0.9316/0.8125 [D-Real: 0.1933 D-Fake: 0.6192]\n",
      "Epoch 005 | ET 0.04 min | Avg Losses >> G/D 0.9662/0.8242 [D-Real: 0.2076 D-Fake: 0.6167]\n",
      "Epoch 006 | ET 0.05 min | Avg Losses >> G/D 1.0578/0.7919 [D-Real: 0.2182 D-Fake: 0.5737]\n",
      "Epoch 007 | ET 0.06 min | Avg Losses >> G/D 1.0119/0.8367 [D-Real: 0.2698 D-Fake: 0.5669]\n",
      "Epoch 008 | ET 0.06 min | Avg Losses >> G/D 1.0362/0.9280 [D-Real: 0.3382 D-Fake: 0.5897]\n",
      "Epoch 009 | ET 0.07 min | Avg Losses >> G/D 0.9935/0.9803 [D-Real: 0.3995 D-Fake: 0.5808]\n",
      "Epoch 010 | ET 0.08 min | Avg Losses >> G/D 0.9567/1.1132 [D-Real: 0.4946 D-Fake: 0.6186]\n",
      "Epoch 011 | ET 0.08 min | Avg Losses >> G/D 1.0139/1.1803 [D-Real: 0.5563 D-Fake: 0.6240]\n",
      "Epoch 012 | ET 0.09 min | Avg Losses >> G/D 0.9589/1.3373 [D-Real: 0.6958 D-Fake: 0.6415]\n",
      "Epoch 013 | ET 0.10 min | Avg Losses >> G/D 0.8403/1.3884 [D-Real: 0.7220 D-Fake: 0.6664]\n",
      "Epoch 014 | ET 0.10 min | Avg Losses >> G/D 0.8739/1.3990 [D-Real: 0.7670 D-Fake: 0.6321]\n",
      "Epoch 015 | ET 0.11 min | Avg Losses >> G/D 0.8749/1.4310 [D-Real: 0.8093 D-Fake: 0.6217]\n",
      "Epoch 016 | ET 0.12 min | Avg Losses >> G/D 0.8738/1.5264 [D-Real: 0.8770 D-Fake: 0.6494]\n",
      "Epoch 017 | ET 0.12 min | Avg Losses >> G/D 0.8414/1.5221 [D-Real: 0.8820 D-Fake: 0.6401]\n",
      "Epoch 018 | ET 0.13 min | Avg Losses >> G/D 0.8583/1.5501 [D-Real: 0.9317 D-Fake: 0.6184]\n",
      "Epoch 019 | ET 0.13 min | Avg Losses >> G/D 0.8265/1.5726 [D-Real: 0.9401 D-Fake: 0.6325]\n",
      "Epoch 020 | ET 0.14 min | Avg Losses >> G/D 0.8309/1.5329 [D-Real: 0.8879 D-Fake: 0.6451]\n",
      "Epoch 021 | ET 0.15 min | Avg Losses >> G/D 0.8431/1.5343 [D-Real: 0.9137 D-Fake: 0.6206]\n",
      "Epoch 022 | ET 0.15 min | Avg Losses >> G/D 0.8170/1.4991 [D-Real: 0.8599 D-Fake: 0.6392]\n",
      "Epoch 023 | ET 0.16 min | Avg Losses >> G/D 0.8114/1.4875 [D-Real: 0.8545 D-Fake: 0.6330]\n",
      "Epoch 024 | ET 0.16 min | Avg Losses >> G/D 0.8295/1.4664 [D-Real: 0.8337 D-Fake: 0.6327]\n",
      "Epoch 025 | ET 0.17 min | Avg Losses >> G/D 0.8282/1.4811 [D-Real: 0.8350 D-Fake: 0.6461]\n",
      "Epoch 026 | ET 0.18 min | Avg Losses >> G/D 0.8421/1.4670 [D-Real: 0.8294 D-Fake: 0.6376]\n",
      "Epoch 027 | ET 0.18 min | Avg Losses >> G/D 0.8099/1.3991 [D-Real: 0.7765 D-Fake: 0.6225]\n",
      "Epoch 028 | ET 0.19 min | Avg Losses >> G/D 0.8087/1.4107 [D-Real: 0.7624 D-Fake: 0.6483]\n",
      "Epoch 029 | ET 0.20 min | Avg Losses >> G/D 0.7956/1.3907 [D-Real: 0.7393 D-Fake: 0.6514]\n",
      "Epoch 030 | ET 0.20 min | Avg Losses >> G/D 0.7857/1.4041 [D-Real: 0.7490 D-Fake: 0.6551]\n",
      "Epoch 031 | ET 0.21 min | Avg Losses >> G/D 0.7874/1.3774 [D-Real: 0.7150 D-Fake: 0.6624]\n",
      "Epoch 032 | ET 0.21 min | Avg Losses >> G/D 0.7819/1.4036 [D-Real: 0.7342 D-Fake: 0.6694]\n",
      "Epoch 033 | ET 0.24 min | Avg Losses >> G/D 0.7602/1.3968 [D-Real: 0.7163 D-Fake: 0.6806]\n",
      "Epoch 034 | ET 0.26 min | Avg Losses >> G/D 0.7252/1.3954 [D-Real: 0.6985 D-Fake: 0.6969]\n",
      "Epoch 035 | ET 0.27 min | Avg Losses >> G/D 0.7434/1.3910 [D-Real: 0.6891 D-Fake: 0.7018]\n",
      "Epoch 036 | ET 0.28 min | Avg Losses >> G/D 0.7059/1.4218 [D-Real: 0.6977 D-Fake: 0.7241]\n",
      "Epoch 037 | ET 0.29 min | Avg Losses >> G/D 0.7187/1.4143 [D-Real: 0.6800 D-Fake: 0.7344]\n",
      "Epoch 038 | ET 0.30 min | Avg Losses >> G/D 0.7045/1.4014 [D-Real: 0.6778 D-Fake: 0.7237]\n",
      "Epoch 039 | ET 0.31 min | Avg Losses >> G/D 0.7139/1.4137 [D-Real: 0.6812 D-Fake: 0.7325]\n",
      "Epoch 040 | ET 0.32 min | Avg Losses >> G/D 0.7264/1.4195 [D-Real: 0.7056 D-Fake: 0.7139]\n",
      "real\n",
      "created training data: 2092 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 8ms/step - loss: 0.5574 - accuracy: 0.7443 - precision_29: 0.8400 - recall_29: 0.5339 - auc_29: 0.7872\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7442747950553894,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 4,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73b9fc5670>,\n",
      " 'number_training_samples': 2092,\n",
      " 'share_real_data': 0.5}\n",
      "\n",
      "================\n",
      "creating data: 0.5 of data boosted: False\n",
      "boosting real data times: 4\n",
      "real\n",
      "created training data: 523 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 13ms/step - loss: 0.5000 - accuracy: 0.7786 - precision_30: 0.8947 - recall_30: 0.5763 - auc_30: 0.8416\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7786259651184082,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73b99388b0>,\n",
      " 'number_training_samples': 523,\n",
      " 'share_real_data': 0.5}\n",
      "\n",
      "================\n",
      "creating data: 0.1 of data boosted: True\n",
      "boosting real data times: 5\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.00 min | Avg Losses >> G/D 0.7646/1.3918 [D-Real: 0.6440 D-Fake: 0.7478]\n",
      "Epoch 002 | ET 0.00 min | Avg Losses >> G/D 0.8591/1.1605 [D-Real: 0.4741 D-Fake: 0.6864]\n",
      "Epoch 003 | ET 0.01 min | Avg Losses >> G/D 0.7544/1.1854 [D-Real: 0.3882 D-Fake: 0.7972]\n",
      "Epoch 004 | ET 0.01 min | Avg Losses >> G/D 0.7907/1.1385 [D-Real: 0.3655 D-Fake: 0.7730]\n",
      "Epoch 005 | ET 0.01 min | Avg Losses >> G/D 0.9034/0.9753 [D-Real: 0.3205 D-Fake: 0.6547]\n",
      "Epoch 006 | ET 0.01 min | Avg Losses >> G/D 0.8302/0.9202 [D-Real: 0.2872 D-Fake: 0.6329]\n",
      "Epoch 007 | ET 0.01 min | Avg Losses >> G/D 0.7674/1.0044 [D-Real: 0.2564 D-Fake: 0.7480]\n",
      "Epoch 008 | ET 0.02 min | Avg Losses >> G/D 0.8557/0.9430 [D-Real: 0.2590 D-Fake: 0.6840]\n",
      "Epoch 009 | ET 0.02 min | Avg Losses >> G/D 0.9065/0.8381 [D-Real: 0.2719 D-Fake: 0.5662]\n",
      "Epoch 010 | ET 0.02 min | Avg Losses >> G/D 0.8564/0.9185 [D-Real: 0.2375 D-Fake: 0.6810]\n",
      "Epoch 011 | ET 0.02 min | Avg Losses >> G/D 0.9018/0.9673 [D-Real: 0.2083 D-Fake: 0.7590]\n",
      "Epoch 012 | ET 0.02 min | Avg Losses >> G/D 0.9826/0.8363 [D-Real: 0.2328 D-Fake: 0.6035]\n",
      "Epoch 013 | ET 0.02 min | Avg Losses >> G/D 1.0420/0.7959 [D-Real: 0.2010 D-Fake: 0.5948]\n",
      "Epoch 014 | ET 0.03 min | Avg Losses >> G/D 0.8782/0.8012 [D-Real: 0.2037 D-Fake: 0.5976]\n",
      "Epoch 015 | ET 0.03 min | Avg Losses >> G/D 0.9295/0.8065 [D-Real: 0.2065 D-Fake: 0.6000]\n",
      "Epoch 016 | ET 0.03 min | Avg Losses >> G/D 0.9131/0.8101 [D-Real: 0.1590 D-Fake: 0.6510]\n",
      "Epoch 017 | ET 0.03 min | Avg Losses >> G/D 0.8531/0.7788 [D-Real: 0.1677 D-Fake: 0.6111]\n",
      "Epoch 018 | ET 0.03 min | Avg Losses >> G/D 0.8590/0.7860 [D-Real: 0.1666 D-Fake: 0.6194]\n",
      "Epoch 019 | ET 0.03 min | Avg Losses >> G/D 0.9751/0.7871 [D-Real: 0.1574 D-Fake: 0.6296]\n",
      "Epoch 020 | ET 0.04 min | Avg Losses >> G/D 0.8827/0.8152 [D-Real: 0.1892 D-Fake: 0.6259]\n",
      "Epoch 021 | ET 0.04 min | Avg Losses >> G/D 0.9631/0.7400 [D-Real: 0.1758 D-Fake: 0.5642]\n",
      "Epoch 022 | ET 0.04 min | Avg Losses >> G/D 0.9436/0.7950 [D-Real: 0.1636 D-Fake: 0.6314]\n",
      "Epoch 023 | ET 0.04 min | Avg Losses >> G/D 0.9171/0.7848 [D-Real: 0.2087 D-Fake: 0.5761]\n",
      "Epoch 024 | ET 0.04 min | Avg Losses >> G/D 0.9806/0.7864 [D-Real: 0.1978 D-Fake: 0.5886]\n",
      "Epoch 025 | ET 0.05 min | Avg Losses >> G/D 0.9529/0.8441 [D-Real: 0.1806 D-Fake: 0.6635]\n",
      "Epoch 026 | ET 0.05 min | Avg Losses >> G/D 0.9124/0.8127 [D-Real: 0.1882 D-Fake: 0.6246]\n",
      "Epoch 027 | ET 0.05 min | Avg Losses >> G/D 0.9502/0.7658 [D-Real: 0.1842 D-Fake: 0.5816]\n",
      "Epoch 028 | ET 0.05 min | Avg Losses >> G/D 0.9141/0.8203 [D-Real: 0.2084 D-Fake: 0.6119]\n",
      "Epoch 029 | ET 0.05 min | Avg Losses >> G/D 0.9506/0.7594 [D-Real: 0.1904 D-Fake: 0.5691]\n",
      "Epoch 030 | ET 0.05 min | Avg Losses >> G/D 0.9761/0.8652 [D-Real: 0.2387 D-Fake: 0.6265]\n",
      "Epoch 031 | ET 0.05 min | Avg Losses >> G/D 0.9878/0.7791 [D-Real: 0.2166 D-Fake: 0.5624]\n",
      "Epoch 032 | ET 0.06 min | Avg Losses >> G/D 1.0046/0.7813 [D-Real: 0.2112 D-Fake: 0.5701]\n",
      "Epoch 033 | ET 0.06 min | Avg Losses >> G/D 1.0060/0.8094 [D-Real: 0.2128 D-Fake: 0.5966]\n",
      "Epoch 034 | ET 0.06 min | Avg Losses >> G/D 0.9838/0.8247 [D-Real: 0.2119 D-Fake: 0.6128]\n",
      "Epoch 035 | ET 0.06 min | Avg Losses >> G/D 1.0391/0.8088 [D-Real: 0.2304 D-Fake: 0.5784]\n",
      "Epoch 036 | ET 0.06 min | Avg Losses >> G/D 1.0583/0.8039 [D-Real: 0.2445 D-Fake: 0.5594]\n",
      "Epoch 037 | ET 0.07 min | Avg Losses >> G/D 0.9957/0.7810 [D-Real: 0.2557 D-Fake: 0.5253]\n",
      "Epoch 038 | ET 0.07 min | Avg Losses >> G/D 1.0160/0.7812 [D-Real: 0.2390 D-Fake: 0.5422]\n",
      "Epoch 039 | ET 0.07 min | Avg Losses >> G/D 1.0768/0.7514 [D-Real: 0.2518 D-Fake: 0.4996]\n",
      "Epoch 040 | ET 0.07 min | Avg Losses >> G/D 1.0869/0.8337 [D-Real: 0.2662 D-Fake: 0.5674]\n",
      "real\n",
      "created training data: 520 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 3s 22ms/step - loss: 0.6042 - accuracy: 0.6870 - precision_31: 0.6875 - recall_31: 0.5593 - auc_31: 0.7648\n",
      "finished grid - results:\n",
      "{'accuracy': 0.6870229244232178,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 5,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73b61a79d0>,\n",
      " 'number_training_samples': 520,\n",
      " 'share_real_data': 0.1}\n",
      "\n",
      "================\n",
      "creating data: 0.1 of data boosted: False\n",
      "boosting real data times: 5\n",
      "real\n",
      "created training data: 104 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 11ms/step - loss: 0.6114 - accuracy: 0.7557 - precision_32: 0.8553 - recall_32: 0.5508 - auc_32: 0.8430\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7557252049446106,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73b6ee6c70>,\n",
      " 'number_training_samples': 104,\n",
      " 'share_real_data': 0.1}\n",
      "\n",
      "================\n",
      "creating data: 0.2 of data boosted: True\n",
      "boosting real data times: 5\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.00 min | Avg Losses >> G/D 0.8724/1.5104 [D-Real: 0.7703 D-Fake: 0.7401]\n",
      "Epoch 002 | ET 0.01 min | Avg Losses >> G/D 0.8670/1.1683 [D-Real: 0.5038 D-Fake: 0.6645]\n",
      "Epoch 003 | ET 0.01 min | Avg Losses >> G/D 0.9161/1.1159 [D-Real: 0.4323 D-Fake: 0.6836]\n",
      "Epoch 004 | ET 0.01 min | Avg Losses >> G/D 1.0402/0.9845 [D-Real: 0.3690 D-Fake: 0.6155]\n",
      "Epoch 005 | ET 0.02 min | Avg Losses >> G/D 1.0384/0.8681 [D-Real: 0.2872 D-Fake: 0.5809]\n",
      "Epoch 006 | ET 0.02 min | Avg Losses >> G/D 1.0698/0.7953 [D-Real: 0.2385 D-Fake: 0.5568]\n",
      "Epoch 007 | ET 0.02 min | Avg Losses >> G/D 1.1050/0.7993 [D-Real: 0.2310 D-Fake: 0.5683]\n",
      "Epoch 008 | ET 0.02 min | Avg Losses >> G/D 1.1641/0.7450 [D-Real: 0.2028 D-Fake: 0.5422]\n",
      "Epoch 009 | ET 0.03 min | Avg Losses >> G/D 1.1904/0.6553 [D-Real: 0.1826 D-Fake: 0.4728]\n",
      "Epoch 010 | ET 0.03 min | Avg Losses >> G/D 1.1858/0.6863 [D-Real: 0.1534 D-Fake: 0.5329]\n",
      "Epoch 011 | ET 0.03 min | Avg Losses >> G/D 1.2631/0.6978 [D-Real: 0.1719 D-Fake: 0.5259]\n",
      "Epoch 012 | ET 0.04 min | Avg Losses >> G/D 1.1583/0.6759 [D-Real: 0.1545 D-Fake: 0.5215]\n",
      "Epoch 013 | ET 0.04 min | Avg Losses >> G/D 1.1532/0.7084 [D-Real: 0.1800 D-Fake: 0.5284]\n",
      "Epoch 014 | ET 0.04 min | Avg Losses >> G/D 1.2043/0.6680 [D-Real: 0.1715 D-Fake: 0.4965]\n",
      "Epoch 015 | ET 0.04 min | Avg Losses >> G/D 1.0787/0.7241 [D-Real: 0.1701 D-Fake: 0.5539]\n",
      "Epoch 016 | ET 0.05 min | Avg Losses >> G/D 1.1192/0.7426 [D-Real: 0.1837 D-Fake: 0.5588]\n",
      "Epoch 017 | ET 0.05 min | Avg Losses >> G/D 1.1304/0.7604 [D-Real: 0.2258 D-Fake: 0.5346]\n",
      "Epoch 018 | ET 0.05 min | Avg Losses >> G/D 1.0990/0.8037 [D-Real: 0.2380 D-Fake: 0.5657]\n",
      "Epoch 019 | ET 0.06 min | Avg Losses >> G/D 1.1081/0.8158 [D-Real: 0.2498 D-Fake: 0.5661]\n",
      "Epoch 020 | ET 0.06 min | Avg Losses >> G/D 1.1153/0.8049 [D-Real: 0.2813 D-Fake: 0.5236]\n",
      "Epoch 021 | ET 0.06 min | Avg Losses >> G/D 1.0549/0.8138 [D-Real: 0.2743 D-Fake: 0.5395]\n",
      "Epoch 022 | ET 0.06 min | Avg Losses >> G/D 1.0936/0.8838 [D-Real: 0.3173 D-Fake: 0.5665]\n",
      "Epoch 023 | ET 0.07 min | Avg Losses >> G/D 1.0100/0.9884 [D-Real: 0.3518 D-Fake: 0.6366]\n",
      "Epoch 024 | ET 0.07 min | Avg Losses >> G/D 1.0695/0.9639 [D-Real: 0.3671 D-Fake: 0.5969]\n",
      "Epoch 025 | ET 0.07 min | Avg Losses >> G/D 1.0634/0.9866 [D-Real: 0.3987 D-Fake: 0.5880]\n",
      "Epoch 026 | ET 0.08 min | Avg Losses >> G/D 0.9857/1.0714 [D-Real: 0.4300 D-Fake: 0.6415]\n",
      "Epoch 027 | ET 0.08 min | Avg Losses >> G/D 1.0457/1.0548 [D-Real: 0.4703 D-Fake: 0.5845]\n",
      "Epoch 028 | ET 0.08 min | Avg Losses >> G/D 0.9633/1.1231 [D-Real: 0.4698 D-Fake: 0.6533]\n",
      "Epoch 029 | ET 0.08 min | Avg Losses >> G/D 1.0772/1.1139 [D-Real: 0.5258 D-Fake: 0.5881]\n",
      "Epoch 030 | ET 0.09 min | Avg Losses >> G/D 1.0033/1.1877 [D-Real: 0.5758 D-Fake: 0.6119]\n",
      "Epoch 031 | ET 0.09 min | Avg Losses >> G/D 1.0324/1.1976 [D-Real: 0.5609 D-Fake: 0.6367]\n",
      "Epoch 032 | ET 0.09 min | Avg Losses >> G/D 1.0300/1.3104 [D-Real: 0.6512 D-Fake: 0.6592]\n",
      "Epoch 033 | ET 0.09 min | Avg Losses >> G/D 0.9616/1.2748 [D-Real: 0.6405 D-Fake: 0.6342]\n",
      "Epoch 034 | ET 0.10 min | Avg Losses >> G/D 0.9928/1.3374 [D-Real: 0.7186 D-Fake: 0.6188]\n",
      "Epoch 035 | ET 0.10 min | Avg Losses >> G/D 0.9223/1.3325 [D-Real: 0.6969 D-Fake: 0.6356]\n",
      "Epoch 036 | ET 0.10 min | Avg Losses >> G/D 0.9672/1.4089 [D-Real: 0.7793 D-Fake: 0.6296]\n",
      "Epoch 037 | ET 0.11 min | Avg Losses >> G/D 0.9342/1.4308 [D-Real: 0.7586 D-Fake: 0.6722]\n",
      "Epoch 038 | ET 0.11 min | Avg Losses >> G/D 0.9570/1.5050 [D-Real: 0.8064 D-Fake: 0.6986]\n",
      "Epoch 039 | ET 0.11 min | Avg Losses >> G/D 0.8749/1.4257 [D-Real: 0.7845 D-Fake: 0.6412]\n",
      "Epoch 040 | ET 0.11 min | Avg Losses >> G/D 0.9542/1.3978 [D-Real: 0.8302 D-Fake: 0.5675]\n",
      "real\n",
      "created training data: 1045 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 9ms/step - loss: 0.5926 - accuracy: 0.7443 - precision_33: 0.7684 - recall_33: 0.6186 - auc_33: 0.7702\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7442747950553894,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 5,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73b177e790>,\n",
      " 'number_training_samples': 1045,\n",
      " 'share_real_data': 0.2}\n",
      "\n",
      "================\n",
      "creating data: 0.2 of data boosted: False\n",
      "boosting real data times: 5\n",
      "real\n",
      "created training data: 209 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 9ms/step - loss: 0.7159 - accuracy: 0.6908 - precision_34: 0.7846 - recall_34: 0.4322 - auc_34: 0.7777\n",
      "finished grid - results:\n",
      "{'accuracy': 0.6908397078514099,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73b10d9dc0>,\n",
      " 'number_training_samples': 209,\n",
      " 'share_real_data': 0.2}\n",
      "\n",
      "================\n",
      "creating data: 0.30000000000000004 of data boosted: True\n",
      "boosting real data times: 5\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.01 min | Avg Losses >> G/D 0.6852/1.4079 [D-Real: 0.5608 D-Fake: 0.8470]\n",
      "Epoch 002 | ET 0.01 min | Avg Losses >> G/D 0.7283/1.1846 [D-Real: 0.4415 D-Fake: 0.7431]\n",
      "Epoch 003 | ET 0.02 min | Avg Losses >> G/D 0.7680/1.0996 [D-Real: 0.3462 D-Fake: 0.7533]\n",
      "Epoch 004 | ET 0.02 min | Avg Losses >> G/D 0.7881/0.9873 [D-Real: 0.2991 D-Fake: 0.6882]\n",
      "Epoch 005 | ET 0.02 min | Avg Losses >> G/D 0.7876/0.9558 [D-Real: 0.2573 D-Fake: 0.6984]\n",
      "Epoch 006 | ET 0.03 min | Avg Losses >> G/D 0.8018/0.9290 [D-Real: 0.2484 D-Fake: 0.6806]\n",
      "Epoch 007 | ET 0.03 min | Avg Losses >> G/D 0.8135/0.8754 [D-Real: 0.2349 D-Fake: 0.6405]\n",
      "Epoch 008 | ET 0.04 min | Avg Losses >> G/D 0.9013/0.8745 [D-Real: 0.2403 D-Fake: 0.6342]\n",
      "Epoch 009 | ET 0.04 min | Avg Losses >> G/D 0.9164/0.8412 [D-Real: 0.2382 D-Fake: 0.6030]\n",
      "Epoch 010 | ET 0.04 min | Avg Losses >> G/D 0.9060/0.8846 [D-Real: 0.2609 D-Fake: 0.6237]\n",
      "Epoch 011 | ET 0.05 min | Avg Losses >> G/D 0.9172/0.8592 [D-Real: 0.2521 D-Fake: 0.6071]\n",
      "Epoch 012 | ET 0.05 min | Avg Losses >> G/D 0.9704/0.8521 [D-Real: 0.2816 D-Fake: 0.5705]\n",
      "Epoch 013 | ET 0.06 min | Avg Losses >> G/D 0.9391/0.8931 [D-Real: 0.2943 D-Fake: 0.5989]\n",
      "Epoch 014 | ET 0.06 min | Avg Losses >> G/D 1.0005/0.9207 [D-Real: 0.3574 D-Fake: 0.5633]\n",
      "Epoch 015 | ET 0.07 min | Avg Losses >> G/D 0.9546/0.9086 [D-Real: 0.3419 D-Fake: 0.5667]\n",
      "Epoch 016 | ET 0.07 min | Avg Losses >> G/D 0.9653/0.9321 [D-Real: 0.3858 D-Fake: 0.5463]\n",
      "Epoch 017 | ET 0.07 min | Avg Losses >> G/D 0.9620/0.9951 [D-Real: 0.3951 D-Fake: 0.6000]\n",
      "Epoch 018 | ET 0.08 min | Avg Losses >> G/D 0.9505/1.0621 [D-Real: 0.4551 D-Fake: 0.6071]\n",
      "Epoch 019 | ET 0.08 min | Avg Losses >> G/D 0.9666/1.0830 [D-Real: 0.4771 D-Fake: 0.6059]\n",
      "Epoch 020 | ET 0.08 min | Avg Losses >> G/D 0.9322/1.0952 [D-Real: 0.4957 D-Fake: 0.5995]\n",
      "Epoch 021 | ET 0.09 min | Avg Losses >> G/D 0.8998/1.1454 [D-Real: 0.5220 D-Fake: 0.6234]\n",
      "Epoch 022 | ET 0.09 min | Avg Losses >> G/D 0.9153/1.2429 [D-Real: 0.5844 D-Fake: 0.6585]\n",
      "Epoch 023 | ET 0.10 min | Avg Losses >> G/D 0.9105/1.2332 [D-Real: 0.6266 D-Fake: 0.6066]\n",
      "Epoch 024 | ET 0.10 min | Avg Losses >> G/D 0.8685/1.2821 [D-Real: 0.6200 D-Fake: 0.6620]\n",
      "Epoch 025 | ET 0.10 min | Avg Losses >> G/D 0.8887/1.3252 [D-Real: 0.6954 D-Fake: 0.6298]\n",
      "Epoch 026 | ET 0.11 min | Avg Losses >> G/D 0.8655/1.2862 [D-Real: 0.6600 D-Fake: 0.6262]\n",
      "Epoch 027 | ET 0.11 min | Avg Losses >> G/D 0.8857/1.3610 [D-Real: 0.7298 D-Fake: 0.6313]\n",
      "Epoch 028 | ET 0.12 min | Avg Losses >> G/D 0.9151/1.4367 [D-Real: 0.8066 D-Fake: 0.6301]\n",
      "Epoch 029 | ET 0.13 min | Avg Losses >> G/D 0.8775/1.4535 [D-Real: 0.8042 D-Fake: 0.6494]\n",
      "Epoch 030 | ET 0.15 min | Avg Losses >> G/D 0.8984/1.4549 [D-Real: 0.8402 D-Fake: 0.6147]\n",
      "Epoch 031 | ET 0.17 min | Avg Losses >> G/D 0.8883/1.4525 [D-Real: 0.8535 D-Fake: 0.5990]\n",
      "Epoch 032 | ET 0.17 min | Avg Losses >> G/D 0.8609/1.4891 [D-Real: 0.8592 D-Fake: 0.6299]\n",
      "Epoch 033 | ET 0.18 min | Avg Losses >> G/D 0.8178/1.5145 [D-Real: 0.8648 D-Fake: 0.6497]\n",
      "Epoch 034 | ET 0.19 min | Avg Losses >> G/D 0.8714/1.5423 [D-Real: 0.8994 D-Fake: 0.6429]\n",
      "Epoch 035 | ET 0.19 min | Avg Losses >> G/D 0.8367/1.5422 [D-Real: 0.8893 D-Fake: 0.6529]\n",
      "Epoch 036 | ET 0.20 min | Avg Losses >> G/D 0.8381/1.5032 [D-Real: 0.8905 D-Fake: 0.6127]\n",
      "Epoch 037 | ET 0.20 min | Avg Losses >> G/D 0.8297/1.5931 [D-Real: 0.9644 D-Fake: 0.6288]\n",
      "Epoch 038 | ET 0.21 min | Avg Losses >> G/D 0.8055/1.5010 [D-Real: 0.8580 D-Fake: 0.6430]\n",
      "Epoch 039 | ET 0.22 min | Avg Losses >> G/D 0.8123/1.4921 [D-Real: 0.8570 D-Fake: 0.6351]\n",
      "Epoch 040 | ET 0.22 min | Avg Losses >> G/D 0.8077/1.5063 [D-Real: 0.8739 D-Fake: 0.6324]\n",
      "real\n",
      "created training data: 1570 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 23ms/step - loss: 0.7154 - accuracy: 0.7328 - precision_35: 0.9138 - recall_35: 0.4492 - auc_35: 0.7810\n",
      "finished grid - results:\n",
      "{'accuracy': 0.732824444770813,\n",
      " 'boosted_data': True,\n",
      " 'data_boosted_x': 5,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73b00a4be0>,\n",
      " 'number_training_samples': 1570,\n",
      " 'share_real_data': 0.30000000000000004}\n",
      "\n",
      "================\n",
      "creating data: 0.30000000000000004 of data boosted: False\n",
      "boosting real data times: 5\n",
      "real\n",
      "created training data: 314 samples total\n",
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "9/9 [==============================] - 1s 10ms/step - loss: 0.5178 - accuracy: 0.7595 - precision_36: 0.7619 - recall_36: 0.6780 - auc_36: 0.8276\n",
      "finished grid - results:\n",
      "{'accuracy': 0.7595419883728027,\n",
      " 'boosted_data': False,\n",
      " 'data_boosted_x': 0,\n",
      " 'model': <keras.engine.sequential.Sequential object at 0x7f73a6bd6190>,\n",
      " 'number_training_samples': 314,\n",
      " 'share_real_data': 0.30000000000000004}\n",
      "\n",
      "================\n",
      "creating data: 0.4 of data boosted: True\n",
      "boosting real data times: 5\n",
      "fitting new generator on smaller data\n",
      "Epoch 001 | ET 0.01 min | Avg Losses >> G/D 0.8756/1.2425 [D-Real: 0.5355 D-Fake: 0.7070]\n",
      "Epoch 002 | ET 0.02 min | Avg Losses >> G/D 0.9032/1.0023 [D-Real: 0.3402 D-Fake: 0.6621]\n",
      "Epoch 003 | ET 0.02 min | Avg Losses >> G/D 0.9288/0.9010 [D-Real: 0.2496 D-Fake: 0.6514]\n",
      "Epoch 004 | ET 0.03 min | Avg Losses >> G/D 0.9905/0.8039 [D-Real: 0.1978 D-Fake: 0.6061]\n",
      "Epoch 005 | ET 0.04 min | Avg Losses >> G/D 0.9589/0.8022 [D-Real: 0.1853 D-Fake: 0.6169]\n",
      "Epoch 006 | ET 0.04 min | Avg Losses >> G/D 1.0049/0.7882 [D-Real: 0.1875 D-Fake: 0.6007]\n",
      "Epoch 007 | ET 0.05 min | Avg Losses >> G/D 0.9463/0.8327 [D-Real: 0.2065 D-Fake: 0.6262]\n",
      "Epoch 008 | ET 0.06 min | Avg Losses >> G/D 1.0228/0.7985 [D-Real: 0.2288 D-Fake: 0.5697]\n",
      "Epoch 009 | ET 0.07 min | Avg Losses >> G/D 1.0135/0.8380 [D-Real: 0.2553 D-Fake: 0.5828]\n",
      "Epoch 010 | ET 0.08 min | Avg Losses >> G/D 1.0026/0.8966 [D-Real: 0.3125 D-Fake: 0.5841]\n"
     ]
    }
   ],
   "source": [
    "from process_data import process_data\n",
    "from enhance_data import enhance_data\n",
    "from nn_gridsearch import nn_gridsearch, make_model\n",
    "from scipy.stats import reciprocal\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "#include_synthetic = False\n",
    "#synthetic_share = 3.0\n",
    "#real_share = 0.2\n",
    "    \n",
    "#data = enhance_data(\n",
    "#    include_synthetic=include_synthetic,\n",
    "#    synthetic_share=synthetic_share,\n",
    "#    real_share=real_share)\n",
    "    \n",
    "testing = process_data()\n",
    "x_test = testing['x_test_processed']\n",
    "y_test = testing['y_test']\n",
    "    \n",
    "grid_parameters = {\n",
    "    'number_hidden_layers': list(range(1, 8)),\n",
    "    'neurons': np.arange(1, 100).tolist(),\n",
    "    'learning_rate': reciprocal(3e-4, 3e-2).rvs(1000).tolist(),\n",
    "    'dropout_rate': np.arange(.2, .6, .1).tolist(),\n",
    "    'alpha': np.arange(.2, .35, .05).tolist(),\n",
    "    'activation': ['elu', 'selu', 'relu']}\n",
    "\n",
    "models_tested = []\n",
    "\n",
    "# boost data progressively bigger to check results\n",
    "for data_boost_x in [.2, .3]:\n",
    "\n",
    "    # progressive rate of the full training set\n",
    "    for data_rate in np.linspace(.3, .8, 6):\n",
    "        \n",
    "        # test each rate with boosted and non-boosted data\n",
    "        for boost in (True, False):            \n",
    "                \n",
    "            print()\n",
    "            print('================')\n",
    "            print(f'creating data: {data_rate} of data boosted: {boost}')\n",
    "            print(f'boosting real data times: {data_boost_x + 1}')\n",
    "    \n",
    "            data = enhance_data(real_share=data_rate, synthetic_share=data_boost_x, include_synthetic=boost)\n",
    "            \n",
    "            print(f'real')\n",
    "            print(f'created training data: {data[\"x_train_processed\"].shape[0]} samples total')\n",
    "            grid = nn_gridsearch(\n",
    "                make_model, \n",
    "                data['x_train_processed'], data['y_train'],\n",
    "                grid_parameters,\n",
    "                n_iterations=20,\n",
    "                verbose=0)\n",
    "            \n",
    "            best_model = grid.best_estimator_.model\n",
    "            stats = best_model.evaluate(x_test, y_test)\n",
    "            \n",
    "            results = {\n",
    "                'model': best_model,\n",
    "                'data_boosted_x': data_boost_x + 1 if boost else 0,\n",
    "                'accuracy': stats[1],\n",
    "                'share_real_data': data_rate,\n",
    "                'boosted_data': boost,\n",
    "                'number_training_samples': data['y_train'].shape[0]}\n",
    "            \n",
    "            print('finished grid - results:')\n",
    "            pprint(results)\n",
    "            models_tested.append(results)\n",
    "    \n",
    "# note = the wrapper takes a FUNCTION as input!\n",
    "#grid = nn_gridsearch(\n",
    "#    make_model, \n",
    "#    data['x_train_processed'], data['y_train'],\n",
    "#    grid_parameters,\n",
    "#    n_iterations=33,\n",
    "#    verbose=0)\n",
    "    \n",
    "#best_model = grid.best_estimator_.model\n",
    "#best_model.evaluate(data['x_test_processed'], data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9659e945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data_boosted_x</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>share_real_data</th>\n",
       "      <th>boosted_data</th>\n",
       "      <th>number_training_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759542</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.3</td>\n",
       "      <td>True</td>\n",
       "      <td>942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767176</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>1254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>1569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.778626</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.759542</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.744275</td>\n",
       "      <td>0.3</td>\n",
       "      <td>True</td>\n",
       "      <td>1256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.767176</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>1672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.744275</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>2092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.778626</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.687023</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.744275</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.690840</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>0.3</td>\n",
       "      <td>True</td>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.759542</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.774809</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>2090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.748092</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>2615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  data_boosted_x  \\\n",
       "0   <keras.engine.sequential.Sequential object at ...               3   \n",
       "1   <keras.engine.sequential.Sequential object at ...               0   \n",
       "2   <keras.engine.sequential.Sequential object at ...               3   \n",
       "3   <keras.engine.sequential.Sequential object at ...               0   \n",
       "4   <keras.engine.sequential.Sequential object at ...               3   \n",
       "5   <keras.engine.sequential.Sequential object at ...               0   \n",
       "6   <keras.engine.sequential.Sequential object at ...               3   \n",
       "7   <keras.engine.sequential.Sequential object at ...               0   \n",
       "8   <keras.engine.sequential.Sequential object at ...               3   \n",
       "9   <keras.engine.sequential.Sequential object at ...               0   \n",
       "10  <keras.engine.sequential.Sequential object at ...               4   \n",
       "11  <keras.engine.sequential.Sequential object at ...               0   \n",
       "12  <keras.engine.sequential.Sequential object at ...               4   \n",
       "13  <keras.engine.sequential.Sequential object at ...               0   \n",
       "14  <keras.engine.sequential.Sequential object at ...               4   \n",
       "15  <keras.engine.sequential.Sequential object at ...               0   \n",
       "16  <keras.engine.sequential.Sequential object at ...               4   \n",
       "17  <keras.engine.sequential.Sequential object at ...               0   \n",
       "18  <keras.engine.sequential.Sequential object at ...               4   \n",
       "19  <keras.engine.sequential.Sequential object at ...               0   \n",
       "20  <keras.engine.sequential.Sequential object at ...               5   \n",
       "21  <keras.engine.sequential.Sequential object at ...               0   \n",
       "22  <keras.engine.sequential.Sequential object at ...               5   \n",
       "23  <keras.engine.sequential.Sequential object at ...               0   \n",
       "24  <keras.engine.sequential.Sequential object at ...               5   \n",
       "25  <keras.engine.sequential.Sequential object at ...               0   \n",
       "26  <keras.engine.sequential.Sequential object at ...               5   \n",
       "27  <keras.engine.sequential.Sequential object at ...               0   \n",
       "28  <keras.engine.sequential.Sequential object at ...               5   \n",
       "29  <keras.engine.sequential.Sequential object at ...               0   \n",
       "\n",
       "    accuracy  share_real_data  boosted_data  number_training_samples  \n",
       "0   0.763359              0.1          True                      312  \n",
       "1   0.755725              0.1         False                      104  \n",
       "2   0.759542              0.2          True                      627  \n",
       "3   0.740458              0.2         False                      209  \n",
       "4   0.740458              0.3          True                      942  \n",
       "5   0.767176              0.3         False                      314  \n",
       "6   0.755725              0.4          True                     1254  \n",
       "7   0.770992              0.4         False                      418  \n",
       "8   0.755725              0.5          True                     1569  \n",
       "9   0.740458              0.5         False                      523  \n",
       "10  0.778626              0.1          True                      416  \n",
       "11  0.759542              0.1         False                      104  \n",
       "12  0.740458              0.2          True                      836  \n",
       "13  0.740458              0.2         False                      209  \n",
       "14  0.744275              0.3          True                     1256  \n",
       "15  0.770992              0.3         False                      314  \n",
       "16  0.767176              0.4          True                     1672  \n",
       "17  0.755725              0.4         False                      418  \n",
       "18  0.744275              0.5          True                     2092  \n",
       "19  0.778626              0.5         False                      523  \n",
       "20  0.687023              0.1          True                      520  \n",
       "21  0.755725              0.1         False                      104  \n",
       "22  0.744275              0.2          True                     1045  \n",
       "23  0.690840              0.2         False                      209  \n",
       "24  0.732824              0.3          True                     1570  \n",
       "25  0.759542              0.3         False                      314  \n",
       "26  0.774809              0.4          True                     2090  \n",
       "27  0.770992              0.4         False                      418  \n",
       "28  0.748092              0.5          True                     2615  \n",
       "29  0.770992              0.5         False                      523  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(models_tested)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a752f59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data_boosted_x</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>share_real_data</th>\n",
       "      <th>boosted_data</th>\n",
       "      <th>number_training_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.759542</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.778626</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.687023</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.690840</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759542</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.744275</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767176</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.759542</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.3</td>\n",
       "      <td>True</td>\n",
       "      <td>942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.744275</td>\n",
       "      <td>0.3</td>\n",
       "      <td>True</td>\n",
       "      <td>1256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>0.3</td>\n",
       "      <td>True</td>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>1254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.767176</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>1672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.774809</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>2090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.778626</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>1569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.744275</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>2092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.748092</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>2615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  data_boosted_x  \\\n",
       "1   <keras.engine.sequential.Sequential object at ...               0   \n",
       "11  <keras.engine.sequential.Sequential object at ...               0   \n",
       "21  <keras.engine.sequential.Sequential object at ...               0   \n",
       "0   <keras.engine.sequential.Sequential object at ...               3   \n",
       "10  <keras.engine.sequential.Sequential object at ...               4   \n",
       "20  <keras.engine.sequential.Sequential object at ...               5   \n",
       "3   <keras.engine.sequential.Sequential object at ...               0   \n",
       "13  <keras.engine.sequential.Sequential object at ...               0   \n",
       "23  <keras.engine.sequential.Sequential object at ...               0   \n",
       "2   <keras.engine.sequential.Sequential object at ...               3   \n",
       "12  <keras.engine.sequential.Sequential object at ...               4   \n",
       "22  <keras.engine.sequential.Sequential object at ...               5   \n",
       "5   <keras.engine.sequential.Sequential object at ...               0   \n",
       "15  <keras.engine.sequential.Sequential object at ...               0   \n",
       "25  <keras.engine.sequential.Sequential object at ...               0   \n",
       "4   <keras.engine.sequential.Sequential object at ...               3   \n",
       "14  <keras.engine.sequential.Sequential object at ...               4   \n",
       "24  <keras.engine.sequential.Sequential object at ...               5   \n",
       "7   <keras.engine.sequential.Sequential object at ...               0   \n",
       "17  <keras.engine.sequential.Sequential object at ...               0   \n",
       "27  <keras.engine.sequential.Sequential object at ...               0   \n",
       "6   <keras.engine.sequential.Sequential object at ...               3   \n",
       "16  <keras.engine.sequential.Sequential object at ...               4   \n",
       "26  <keras.engine.sequential.Sequential object at ...               5   \n",
       "9   <keras.engine.sequential.Sequential object at ...               0   \n",
       "19  <keras.engine.sequential.Sequential object at ...               0   \n",
       "29  <keras.engine.sequential.Sequential object at ...               0   \n",
       "8   <keras.engine.sequential.Sequential object at ...               3   \n",
       "18  <keras.engine.sequential.Sequential object at ...               4   \n",
       "28  <keras.engine.sequential.Sequential object at ...               5   \n",
       "\n",
       "    accuracy  share_real_data  boosted_data  number_training_samples  \n",
       "1   0.755725              0.1         False                      104  \n",
       "11  0.759542              0.1         False                      104  \n",
       "21  0.755725              0.1         False                      104  \n",
       "0   0.763359              0.1          True                      312  \n",
       "10  0.778626              0.1          True                      416  \n",
       "20  0.687023              0.1          True                      520  \n",
       "3   0.740458              0.2         False                      209  \n",
       "13  0.740458              0.2         False                      209  \n",
       "23  0.690840              0.2         False                      209  \n",
       "2   0.759542              0.2          True                      627  \n",
       "12  0.740458              0.2          True                      836  \n",
       "22  0.744275              0.2          True                     1045  \n",
       "5   0.767176              0.3         False                      314  \n",
       "15  0.770992              0.3         False                      314  \n",
       "25  0.759542              0.3         False                      314  \n",
       "4   0.740458              0.3          True                      942  \n",
       "14  0.744275              0.3          True                     1256  \n",
       "24  0.732824              0.3          True                     1570  \n",
       "7   0.770992              0.4         False                      418  \n",
       "17  0.755725              0.4         False                      418  \n",
       "27  0.770992              0.4         False                      418  \n",
       "6   0.755725              0.4          True                     1254  \n",
       "16  0.767176              0.4          True                     1672  \n",
       "26  0.774809              0.4          True                     2090  \n",
       "9   0.740458              0.5         False                      523  \n",
       "19  0.778626              0.5         False                      523  \n",
       "29  0.770992              0.5         False                      523  \n",
       "8   0.755725              0.5          True                     1569  \n",
       "18  0.744275              0.5          True                     2092  \n",
       "28  0.748092              0.5          True                     2615  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['share_real_data', 'data_boosted_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1fb8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./boosting_results.csv', mode='w') as file:\n",
    "    df.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9ce3b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data_boosted_x</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>share_real_data</th>\n",
       "      <th>boosted_data</th>\n",
       "      <th>number_training_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.778626</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.687023</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.759542</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.744275</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767176</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.3</td>\n",
       "      <td>True</td>\n",
       "      <td>942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.744275</td>\n",
       "      <td>0.3</td>\n",
       "      <td>True</td>\n",
       "      <td>1256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>0.3</td>\n",
       "      <td>True</td>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>1254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.767176</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>1672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.774809</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>2090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>1569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.744275</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>2092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.748092</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>2615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  data_boosted_x  \\\n",
       "1   <keras.engine.sequential.Sequential object at ...               0   \n",
       "0   <keras.engine.sequential.Sequential object at ...               3   \n",
       "10  <keras.engine.sequential.Sequential object at ...               4   \n",
       "20  <keras.engine.sequential.Sequential object at ...               5   \n",
       "3   <keras.engine.sequential.Sequential object at ...               0   \n",
       "2   <keras.engine.sequential.Sequential object at ...               3   \n",
       "12  <keras.engine.sequential.Sequential object at ...               4   \n",
       "22  <keras.engine.sequential.Sequential object at ...               5   \n",
       "5   <keras.engine.sequential.Sequential object at ...               0   \n",
       "4   <keras.engine.sequential.Sequential object at ...               3   \n",
       "14  <keras.engine.sequential.Sequential object at ...               4   \n",
       "24  <keras.engine.sequential.Sequential object at ...               5   \n",
       "7   <keras.engine.sequential.Sequential object at ...               0   \n",
       "6   <keras.engine.sequential.Sequential object at ...               3   \n",
       "16  <keras.engine.sequential.Sequential object at ...               4   \n",
       "26  <keras.engine.sequential.Sequential object at ...               5   \n",
       "9   <keras.engine.sequential.Sequential object at ...               0   \n",
       "8   <keras.engine.sequential.Sequential object at ...               3   \n",
       "18  <keras.engine.sequential.Sequential object at ...               4   \n",
       "28  <keras.engine.sequential.Sequential object at ...               5   \n",
       "\n",
       "    accuracy  share_real_data  boosted_data  number_training_samples  \n",
       "1   0.755725              0.1         False                      104  \n",
       "0   0.763359              0.1          True                      312  \n",
       "10  0.778626              0.1          True                      416  \n",
       "20  0.687023              0.1          True                      520  \n",
       "3   0.740458              0.2         False                      209  \n",
       "2   0.759542              0.2          True                      627  \n",
       "12  0.740458              0.2          True                      836  \n",
       "22  0.744275              0.2          True                     1045  \n",
       "5   0.767176              0.3         False                      314  \n",
       "4   0.740458              0.3          True                      942  \n",
       "14  0.744275              0.3          True                     1256  \n",
       "24  0.732824              0.3          True                     1570  \n",
       "7   0.770992              0.4         False                      418  \n",
       "6   0.755725              0.4          True                     1254  \n",
       "16  0.767176              0.4          True                     1672  \n",
       "26  0.774809              0.4          True                     2090  \n",
       "9   0.740458              0.5         False                      523  \n",
       "8   0.755725              0.5          True                     1569  \n",
       "18  0.744275              0.5          True                     2092  \n",
       "28  0.748092              0.5          True                     2615  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['share_real_data', 'boosted_data', 'data_boosted_x']).sort_values(['share_real_data', 'data_boosted_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc4b78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "edb24b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f7438f6e2e0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1515.38x2160 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(\n",
    "    x='data_boosted_x', \n",
    "    hue='boosted_data', \n",
    "    y='accuracy', \n",
    "    data=df, \n",
    "    col='share_real_data',\n",
    "    kind='bar',\n",
    "    col_wrap=2,\n",
    "    height=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab087992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
