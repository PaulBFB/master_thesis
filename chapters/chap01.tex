\chapter{Introduction \& Problem Statement}

In 2012 Krizhevsky and his colleagues entered and won the ImageNet classification contest with a deep convolutional neural network \ac{cnn}, outperforming other models by a significant margin \cite{krizhevsky2012imagenet}. This marked a turning point in machine learning in general, and in perceptual tasks specifically. 

\cite{norvig_eod} is often invoked as a shorthand to the core problem of Machine Learning, the fact that a larger training corpus would always be preferable.

Currently, data scientists spend a significant amount (how much? sources!) of their time, when solving 'shallow' machine learning tasks (such as???) in feature engineering / preprocessing. Source! examples!
This is due to the fact that shallow approaches such as decision trees, GBM and SVM models require features that 'directly' connect the prepared data to the searched-for outcome. (source) 

Deep learning (neural networks) create intermediate representations via \textbf{stacked layers} at the cost of increased training data (source). 
Thereby enabling a more abstract understanding of patterns within the data.

\citet{Shearer2000}

\section{Objectives}


\section{Methods}


\section{Structure}

%\blindtext

\section{Tables}

Table~\ref{tab:table-one} shows an example table.

\begin{table}[htbp]
    \centering
    \caption{This is a table}
    \label{tab:table-one}
    \begin{tabular}{lll}
        \addlinespace
        \toprule
        Column 1 & Column 2 & Column 3 \\
        \midrule
        A     & B     & C \\
        D     & E     & F \\
        G     & H     & I \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Source Code}

