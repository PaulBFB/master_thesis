\chapter{Technical Application}

\section{Theoretical applicability}

In their landmark paper in 2014, \cite{goodfellow2014generative} demonstrated the viability of \acp{GAN} on creating image data on the classic MNIST dataset (\cite{mnist}), by generating - among other things - convincing handwritten digits.
As mentioned in \ref{enhace_imagedata}, some of the architecture specifics and evaluation are quite specific to image data in that 

\begin{itemize}
	\item the data contains a notion of locality, as neighboring data points (i.e. pixels) are strongly dependent
	\item dimensionality of the generated data is higher than the \textbf{latent space} 
	\item results lend themselves to visual quality inspection by humans (it is easy to see even degrees of quality between different architectures)
\end{itemize}

specifically the former points are strongly relevant to \ac{GAN} architecture, as will become obvious shortly.

\section{Technical implementation steps}

Since the goal of this paper is to evaluate whether or not \ac{GAN} may be used to not only generate more data of a small non-image dataset (which is fairly trivial) but whether or not this data actually serves to \textbf{boost model performance} of models trained on the resulting data, a small, well-understood standard dataset was used to develop the initial architecture; \cite{titanic}.
Specifically, the iconic titanic dataset constitutes a binary classification problem, which facilitates quick model evaluation and ameliorates some of the more typical difficulties of training \acp{GAN} - see below.

The first attempts to create a basic, dense \ac{GAN} actually failed to converge for a significant number of experiments with different amounts of layers, neurons and size of the latent space. Somewhat unsurprisingly, achieving the classic Nash Equilibrium between discriminator and generator was fairly difficult and the initial models all proved unstable. \acp{GAN} provide several unique challenges, and/or failure modes:

\begin{itemize}
	\item mode collapse \cite{mode_collapse}
	\item oscillation and general instability of the model \cite{gan_continual_learning}
	\item catastrophic forgetting \cite{catastrophic_forgetting}
\end{itemize}

Mode collapse is especially relevant in a task like MNIST, where there are multiple classes to be generated, and the generator becomes increasingly proficient in generating one class explicitly - thankfully, this is less of an issue in a binary classification task.

\subsection{Network Architecture}

The other failure modes, however \textbf{did} all make an appearance at one time or another, after the initial data preparation. It was fairly clear that the initial network, with one layer each for the generator and the discriminator each, and 64 neurons had insufficient representational power to converge on creating convincing samples as can be seen in \ref{fig:gan1}:

\fig{img/gan_insufficient_power}{Initial simple dense GAN - left side shows the losses of generator and discriminator, right side shows the probabilities assigned to real and fake samples by the discriminator}{fig:gan1}{.8}

\pagebreak 

Further experiments, with increased numbers of layers and neurons, produced first a very textbook oscillation pattern, shown in \ref{fig:gan2}:
 
\fig{img/gan_oscillation_2}{Dense GAN, 3 layers, 64 neurons/layer; left - losses of generator/discriminator right - the probabilities real/fake assigned by the discriminator}{fig:gan2}{.8}

\pagebreak 

Finally, it has to be stressed that finding the ideal combination learning rates, dropout in the discriminator and number of training epochs, is really quite difficult, especially since there appears no good substitute to visually examining the pattern that is produced by a given architecture and then to adjust. A process that has to be iterated for quite a while, and is fairly manual and heavy on trial-and-error.
 
Ultimately, a a promising architecture appeared to be dense networks with 3 layers each, but a higher number of neurons, and still these networks diverged rather quickly shown here \ref{fig:gan3}:
 
\fig{img/gan_divergence_3_layers_128_units.png}{Dense GAN, 3 layers, 128 neurons/layer, reduced learning rate and dropout in discriminator - losses of generator/discriminator right - the probabilities real/fake assigned by the discriminator}{fig:gan3}{.8}

Unfortunately, from here on out simply optimizing the number of neurons and learning rate and learning rate scheduling was not enough to mitigate this divergence. Although implementing the popular 1Cycle learning rate decay (described by \cite{smith_1cycle}) did ameliorate the issue somewhat, it did not fix the network.

\pagebreak

What ultimately made the difference is an adaptation of the architecture proposed by \cite{dcgan}. The architecture proposed here for image generation constitutes a \textbf{symmetrical} upsampling from the latent space in the generator (in case of images, a \textbf{transposed convolution}) and downsampling in the discriminator. As shown by \cite{oversampling_gan} here:

\fig{img/convolutional_architecture.png}{Architecture of Discriminator and Generator}{fig:gan4}{.8}

Initially, implementing convolution actually deteriorated performance and completely prevented convergence of the network, a probable explanation would be the fact that convolution and transposed convolution not only upsample the latent space but more fundamentally relate to locality in the data; i.e. multiple convolutional layers over a picture effectively create hierarchical feature extraction. A paper that illustrates the mechanics of this fairly well was \cite{convolution_arithmetic}. Effectively, these convolutions would initially find small features in images, subsequent convolutions would assemble these features into feature maps and their presence would indicate the presence of objects in an image. The entire concept of strides and adjacent data points however, does not make sense in the concept of a dataset where an observation consists of a feature vector, in which the order does not convey any information. While 1D convolutions are quite widely used in sequence and time-series processing - which are quite comfortably out of scope of this paper - they fundamentally seem unsuited to a dataset which would not lose any information if the order of its' attributes was permutated.

What \textbf{did} make a difference was implementing the symmetry of upsampling and condensing in the generator and discriminator.

Furthermore, \cite{dcgan} propose other guidelines for building \acp{DCGAN} which proved helpful:

\begin{itemize}
	\item implementing BatchNormalization in the generator and discriminator \cite{batchnorm}
	\item using ReLU activation in all layers in the generator except for the output, which would use tanh
	\item using LeakyReLU in all layers in the discriminator, except for the output which uses sigmoid
\end{itemize}

After implementing these guidelines, using Binary Categorical Crossentropy loss, the generator and discriminator actually converged fairly well already, as seen in \ref{fig:gan5}

\fig{img/best_dense_generator}{Dense GAN, 2 layers, 32 neurons/layer; left - losses of generator/discriminator right - the probabilities real/fake assigned by the discriminator}{fig:gan5}{.8}

Importantly, in this architecture the \textbf{generator} model starts with a dense layer containing 32 neurons, which doubles in every layer (once in this case, although this is a variable parameter). The final dense hidden layer is then downsampled again to reflect the original data - like this:

\lstinputlisting[language=Python, basicstyle=\tiny, caption=generator network, label=lst:generator]{./lst/create_generator_network.py}
Listing~\ref{lst:generator} shows the python function which creates the generator network.

\pagebreak 

The discriminator implements the exact mirror image of this pattern, beginning with the same amount of neurons after the input layer and downsampling by half each layer; 

\lstinputlisting[language=Python, basicstyle=\tiny, caption=discriminator network, label=lst:discriminator]{./lst/create_discriminator_network.py}
Listing~\ref{lst:discriminator} shows the python function which creates the discriminator network.

\pagebreak 

\subsection{Network Training Implementation}

With the basic architecture for the Network in place, the functions are put together into a custom training loop. While there are multiple approaches to training \acp{GAN}, starting with the seminal paper by \cite{goodfellow2014generative}, such as training the discriminator and generator in an epoch separately, here a custom training loop with separate optimizers was chosen from the beginning, in order to accommodate more exotic loss functions.

Since a number of experiments on the efficacy of the generated data has to be done, the entire \ac{GAN} system was set up to be set up with sensible defaults, dynamically adapting to 1D datasets. 
Specifically \cite{buitinck2013api} suggest design lessons from scikit-learn, one of which is sensible defaults.

In order to automatically create both generators and discriminators dynamically based on input shape (but with strong default settings which ideally do not have to be adjusted during experimentation at all) a small package was created which encapsulates the entire training loop.

Key part here is the training loop shown here:

\newpage

\lstinputlisting[language=Python, basicstyle=\tiny, caption=training loop, label=lst:train_loop]{./lst/train_generator_simplified.py}

Listing~\ref{lst:train_loop} shows the python function which trains the network. Note that this function is \textbf{quite strongly simplified}, 
the actual code used can be found at \url{https://github.com/PaulBFB/master_thesis/blob/main/train_generator.py} and would not likely be germane to the paper in its' entirety in any case.

\pagebreak

The above training loop was developed together with the network architecture and also produced the training graphics shown so far. Before it was used in experimentation however, adjusting its' loss function was tested.
Specifically, as proposed by \cite{arjovsky2017wasserstein}, implementing the \ac{EM} as a loss function. 

\fig{img/wasserstein_distance}{Wasserstein distance formula}{fig:wasserstein}{.5}

As mentioned in the paper, this distance denotes the amount of work that is necessary to transform one distribution in to another, given an optimal transfer plan $\gamma$ which actually denotes \textbf{how} the work is done.
Furthermore, and probably most importantly, the \ac{EM}, in contrast to other loss functions, is actually a function of the parameters $\theta$ of the distributions in question, i.e. it can express partial derivatives with respect to the single parameters!

However, finding $\gamma$ is an optimization problem by itself, since it constitutes an \textbf{optimal} solution. As \cite{arjovsky2017wasserstein} mention therefore, it is approximated during training.
A complete explication of the metric and its' approximation is out of scope of this paper.

What this achieves in practice, is that it enables the discriminator to act as a \textbf{critic}, essentially reporting the distance that the generator has yet to move back during training, 
which the generator then backpropagates to its' parameters. Thereby,the loss during training actually becomes more meaningfully readable.

\pagebreak

\cite{arjovsky2017wasserstein} recommend in their paper to clip the gradients reported back to the generator to be clipped. This led to substantial instability in the network, as can be seen here:

\fig{img/wasserstein_3_layers_unstable}{Wasserstein GAN, 3 layers; left - losses of generator/discriminator right - the probabilities real/fake assigned by the discriminator}{fig:wasserstein_train}{.8}

As can be seen clearly here, the equilibrium between the components is fairly unstable.
\cite{gulrajani2017improved} pose that gradient clipping here actually leads to exploding and vanishing gradients, which seems to describe this result fairly well. 
In the actual implementation therefore, the \textbf{Gradient Penalty} method they recommend is implemented, namely:

\begin{itemize}
	\item between real and fake examples in a batch, choose a random number sampled from a uniform distribution
	\item interpolate between real and fake examples
	\item calculate discriminator loss for all interpolated examples
	\item add the gradient penalty based on the interpolations
	\item remove batch normalization from the discriminator, since it shifts example gradients based on the entire batch
\end{itemize}

\pagebreak

Therefore, the training loop was modified:

\lstinputlisting[language=Python, basicstyle=\tiny, caption=training loop, label=lst:train_loop_wasserstein]{./lst/train_generator_wasserstein_simplified.py}

Note that according to \cite{gulrajani2017improved} a $\lambda$ value of 10.0 worked well in all examples, which is what was used here. Again, this is a strongly truncated version of the code, the full version can be found at \url{https://github.com/PaulBFB/master_thesis/blob/main/train_wasserstein_generator.py} - this also contains modified functions to create a \textbf{discriminator} without BatchNormalization.

The resulting training loop with the same layers and upsampling-downsampling symmetry between generator and discriminator resulted in this:

\fig{img/wasserstein_2layers_50_epochs_promising}{Wasserstein GAN with gradient penalty, 2 layers; left - losses of generator/discriminator right - the probabilities real/fake assigned by the discriminator}{fig:wasserstein_gp_train}{.8}

The equilibrium is quite tightly clustered around a probability of real and fake samples, which indicates that the discriminator and generator quickly reach a stable equilibrium 
in which the discriminator is essentially forced to guess between real and fake examples.

Both training loops, the basic DCGAN-adapted including BatchNormalization as well as the Wasserstein loop including penalty were preserved as separate modules. 
In order to perform efficient experiments on entire datasets, again with sensible default values as recommended by \cite{buitinck2013api} those loops were wrapped into a python module, which enables:

\begin{itemize}
	\item accept preprocessed data in the form of numpy arrays, keep the original size or take a random sample from it (in order to experiment with even smaller data-subsets)
	\item retrain either a Wasserstein-GP or DCGAN-adapted generator if the data was decreased (or the generator is forced to be trained)
	\item create a number of samples based on the amount of original data and mix it into the original data
\end{itemize}

Especially important here is the second point, 
