\chapter{Technical Application}

\section{Theoretical applicability}

In their landmark paper in 2014, \cite{goodfellow2014generative} demonstrated the viability of \acp{GAN} on creating image data on the classic MNIST dataset (\cite{mnist}), by generating - among other things - convincing handwritten digits.
As mentioned in \ref{enhace_imagedata}, some of the architecture specifics and evaluation are quite specific to image data in that 

\begin{itemize}
	\item the data contains a notion of locality, as neighboring data points (i.e. pixels) are strongly dependent
	\item dimensionality of the generated data is higher than the \textbf{latent space} 
	\item results lend themselves to visual quality inspection by humans (it is easy to see even degrees of quality between different architectures)
\end{itemize}

specifically the former points are strongly relevant to \ac{GAN} architecture, as will become obvious shortly.

\section{Technical implementation steps}

Since the goal of this paper is to evaluate whether or not \ac{GAN} may be used to not only generate more data of a small non-image dataset (which is fairly trivial) but whether or not this data actually serves to \textbf{boost model performance} of models trained on the resulting data, a small, well-understood standard dataset was used to develop the initial architecture; \cite{titanic}.
Specifically, the iconic titanic dataset constitutes a binary classification problem, which facilitates quick model evaluation and ameliorates some of the more typical difficulties of training \acp{GAN} - see below.

The first attempts to create a basic, dense \ac{GAN} actually failed to converge for a significant number of experiments with different amounts of layers, neurons and size of the latent space. Somewhat unsurprisingly, achieving the classic Nash Equilibrium between discriminator and generator was fairly difficult and the initial models all proved unstable. \acp{GAN} provide several unique challenges, and/or failure modes:

\begin{itemize}
	\item mode collapse \cite{mode_collapse}
	\item oscillation and general instability of the model \cite{gan_continual_learning}
	\item catastrophic forgetting \cite{catastrophic_forgetting}
\end{itemize}

Mode collapse is especially relevant in a task like MNIST, where there are multiple classes to be generated, and the generator becomes increasingly proficient in generating one class explicitly - thankfully, this is less of an issue in a binary classification task.

\subsection{Network Architecture}

The other failure modes, however \textbf{did} all make an appearance at one time or another, after the initial data preparation. It was fairly clear that the initial network, with one layer each for the generator and the discriminator each, and 64 neurons had insufficient representational power to converge on creating convincing samples as can be seen in \ref{fig:gan1}:

\fig{img/gan_insufficient_power}{Initial simple dense GAN - left side shows the losses of generator and discriminator, right side shows the probabilities assigned to real and fake samples by the discriminator}{fig:gan1}{.8}

Further experiments, with increased numbers of layers and neurons, produced first a very textbook oscillation pattern, shown in \ref{fig:gan2}:
 
\fig{img/gan_oscillation_2}{Dense GAN, 3 layers, 64 neurons/layer; left - losses of generator/discriminator right - the probabilities real/fake assigned by the discriminator}{fig:gan2}{.8}

\pagebreak 
Finally, it has to be stressed that finding the ideal combination learning rates, dropout in the discriminator and number of training epochs, is really quite difficult, especially since there appears no good substitute to visually examining the pattern that is produced by a given architecture and then to adjust. A process that has to be iterated for quite a while, and is fairly manual and heavy on trial-and-error.
 
Ultimately, a a promising architecture appeared to be dense networks with 3 layers each, but a higher number of neurons, and still these networks diverged rather quickly shown here \ref{fig:gan3}:
 
\fig{img/gan_divergence_3_layers_128_units.png}{Dense GAN, 3 layers, 128 neurons/layer, reduced learning rate and dropout in discriminator - losses of generator/discriminator right - the probabilities real/fake assigned by the discriminator}{fig:gan3}{.8}

What ultimately made the difference is an adaptation of the architecture proposed by \cite{dcgan}. The architecture proposed here for image generation constitutes a \textbf{symmetrical} upsampling from the latent space in the generator (in case of images, a \textbf{transposed convolution}) and downsampling in the discriminator. As shown by \cite{oversampling_gan} here:


