\chapter{Discussion}

\section{General Thoughts}

Here the general outcomes from every aspect of the thesis will be discussed.
The initial motivation here was to evaluate whether data augmentation of small datasets with \acp{GAN} is a feasible low-barrier method to increasing model performance, maybe even an alternative to collecting additional data and/or extensive feature engineering.
While algorithmic approaches such as \ac{bagging}, as described by \cite{bagging} offer substantial performance boosts, it seems that even an apparently stable \ac{GAN} - while able to produce superficially similar examples to the initial data distribution - does not actually help to extract additional \textbf{meaning} from small sets of 1-dimensional data.

There's a quote, probably apocryphal but fairly apropos here;

\begin{center} 
	
	\textbf{“I have not failed. I’ve just found 10,000 ways that won’t work.”}
	
\end{center}

While certainly not thousands of ways, quite a few were found.

However, as will be noted in the following sections, some of the results may still be of value. Specifically I suspect the adaptation of the symmetrical \ac{GAN} upsampling-downsampling architecture of generator and discriminator may find some use.

\pagebreak

\section{Conclusions from architecture and technical design}

First of all, it really cannot be stressed enough how helpful and impactful the design principles in scikit-learn described by \cite{buitinck2013api} were. 

Both in the implementation of the experiments in (specifically, the module implementing neural network gridsearch "bridging" scikit-learn and keras here \ref{lst:gridsearch}) repeatedly shrinking and boosting the data substantially accelerated the development process.

The guidelines also served as a sort of guardrail to design experiment processes along. 
Successfully creating a \ac{GAN} system without any additional parameters that appears to - somewhat - establish equilibrium on arbitrary new training data was certainly a helpful milestone.

\ac{GAN} architecture itself was comfortably the biggest challenge during the project, simply because 
\begin{itemize}
	\item one can never be certain whether or not the network is not converging because the correct configuration of 
	\begin{itemize}
		\item layers
		\item neurons
		\item learning rate / learning rate scheduling
		\item activation function(s)
		\item number of epochs
	\end{itemize} has simply not been found yet, or if there is no such architecture for just this case
	\item whether or not the network has established a semi-stable equilibrium yet is difficult to check for automatically (at least, no universal method has been established in the literature)
	\item the quality of non-image data generated by a \ac{GAN} is generally difficult to assess beyond a simple check whether or not the values fall within the parameters of the attribute that is being modeled.
\end{itemize}

To expand on the last point, since a lot of literature concerning Generative Adversarial Networks is focused on image generation, a task in which humans still have largely unchallenged \textbf{general} capacity, a common identification of failure modes (such as mode collapse described by \cite{mode_collapse}) is simply "look at it". In the case of vector data it becomes fairly intractable to quickly and informally assess whether the generator network has been stuck in generating a certain subset of the vector space.

This is actually a critical point; in the example of data generation on new datasets, it quickly became obvious that balance of the dataset is a key factor in model performance. This intuitively makes sense, since a generator that was stuck in a subset of the vector space would wildly unbalance any training data that is synthetically increased with it.

There may be applications in using Conditional \acp{GAN} as described by \cite{conditional_gans} in order to create stratified synthetic training data; an approach that would actually be somewhat analogous to \ac{SMOTE} as described by \cite{smote}.

Using multiple wrapper modules to generate different data shrink and boost sizes, run gridsearch on them and record the results (as well as any resulting \ac{GAN} generator training logs) proved as an involved but ultimately fairly efficient process to thoroughly evaluate the effect on predictive power.

\pagebreak

\subsection{Gridsearch}

Specifically concerning gridsearch performed on neural networks using the scikit-learn wrapper as mentioned here \ref{lst:gridsearch}, while generally a promising design pattern that can definitely be reused, ultimately proved to be fairly time consuming (due to the duration of the experiments, fitting a large number of networks using Crossvalidation and selecting the best one). 

Furthermore, a byproduct of the specific gridsearch chosen, randomized gridsearch, was some amount of randomness in the ultimate performance of the models, which made interpretation of these results more difficult. 

This is also the reason it was not actually applied to the new datasets, as it would likely do nothing but obfuscate the results - which are now thankfully (if personally disappointingly) plain.

\pagebreak

\section{Conclusions from shrinking and boosting the original dataset}

\pagebreak

\section{Conclusions from application to other datasets}

\pagebreak

\section{Conclusions from complete data replacement}

\cite{ares_utility}

\pagebreak

\section{Final Thoughts}

\pagebreak
