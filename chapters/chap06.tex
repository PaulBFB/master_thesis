\chapter{Discussion}
\label{chapter:discussion}

\section{General Thoughts}

Here the general outcomes from every aspect of the thesis will be discussed.

The initial motivation was to evaluate whether data augmentation of small datasets with \acp{GAN} is a feasible low-barrier method to increasing model performance, maybe even an alternative to collecting additional data and/or extensive feature engineering.

While algorithmic approaches such as \ac{bagging}, as described by \cite{bagging} offer substantial performance boosts, it seems that even an apparently stable \ac{GAN} - while able to produce superficially similar examples to the initial data distribution - does not actually help to extract additional \textbf{meaning} from small sets of 1-dimensional data.

There's a quote, probably apocryphal but fairly apropos here;

\begin{center} 
	
	\textbf{“I have not failed. I’ve just found 10,000 ways that won’t work.”}
	
\end{center}

While certainly not thousands of ways, quite a few were found.

However, as will be noted in the following sections, some of the results may still be of value. Specifically I suspect the adaptation of the symmetrical \ac{GAN} upsampling-downsampling architecture of generator and discriminator may find some use.

\pagebreak

\section{Conclusions from architecture and technical design}

First of all, it really cannot be stressed enough how helpful and impactful the design principles in scikit-learn described by \cite{buitinck2013api} were. 

Both in the implementation of the experiments in repeatedly shrinking and boosting the data, as well as in the module implementing neural network gridsearch "bridging" scikit-learn and keras (here \ref{lst:gridsearch}) using these guidelines substantially accelerated the development and experimentation process since it made the practice of repeatedly creating the standard \ac{GAN}, boosting data and mixing it into existing training data much quicker.

The guidelines also served as a sort of guardrail to design experiment processes along of. 

Successfully creating a \ac{GAN} system without any additional parameters that appears to - somewhat - establish equilibrium on arbitrary new training data was certainly a helpful milestone.

\pagebreak

\ac{GAN} architecture itself was comfortably the biggest challenge during the project, simply because 
\begin{itemize}
	\item one can never be certain whether or not the network is not converging because the correct configuration of 
	\begin{itemize}
		\item layers
		\item neurons
		\item learning rate / learning rate scheduling
		\item activation function(s)
		\item dimension and type (uniform versus normal) of the latent space
		\item number of epochs
	\end{itemize} has simply not been found yet, or if there is no such architecture for just this case
	\item whether or not the network has established a semi-stable equilibrium yet is difficult to check for automatically (at least, no universal method has been established in the literature)
	\item the quality of non-image data generated by a \ac{GAN} is generally difficult to assess beyond a simple check whether or not the values fall within the parameters of the attribute that is being modeled.
\end{itemize}

To expand on the last point, since a lot of literature concerning Generative Adversarial Networks is focused on image generation, a task in which humans still have largely unchallenged \textbf{general} capacity, a common identification of failure modes (such as mode collapse described by \cite{mode_collapse}) is simply "look at it". 

In the case of data in the form of attribute vectors it becomes fairly intractable to quickly and informally assess whether the generator network has been stuck in generating a certain subset of the sampling space.

This is actually a critical point; in the example of data generation on new datasets, it quickly became obvious that balance of the dataset is a key factor in model performance. This intuitively makes sense, since a generator that was stuck in a subset of the vector space would wildly unbalance any training data that is synthetically increased with it or increase any existing (even slight) imbalance.

There may be applications in using Conditional \acp{GAN} as described by \cite{conditional_gans} in order to create stratified synthetic training data; an approach that would actually be somewhat analogous to \ac{SMOTE} as described by \cite{smote} which would serve to preserve the ratio of classes from the original data.

Using multiple wrapper modules to generate different data shrink and boost sizes, run gridsearch on them and record the results (as well as any resulting \ac{GAN} generator training logs) proved as an involved but ultimately fairly efficient process to thoroughly evaluate the effect on predictive power.

\pagebreak

\subsection{Gridsearch}

Specifically concerning gridsearch performed on neural networks using the scikit-learn wrapper (as mentioned here \ref{lst:gridsearch}), while generally a promising design pattern that can definitely be reused, ultimately proved to be fairly time consuming (due to the duration of the experiments, fitting a large number of networks using Crossvalidation and selecting the best one). 

Furthermore, a byproduct of the specific gridsearch chosen, randomized gridsearch, was some amount of randomness in the ultimate performance of the models, which made interpretation of these results more difficult. 

This is also the reason it was not actually applied to the new datasets, as it would likely do nothing but obfuscate the results - which are now thankfully (if personally disappointingly) plain.

\pagebreak

\section{Conclusions from shrinking and boosting the original dataset}

In this section, the approach that was chosen actually showed the most promise. 

Interestingly enough, model performance never degraded at all. While it was strange to see that the model hardly suffered from decreased data size, the results on all datasets here seem to indicate that this \ac{GAN} architecture is aligned to the specific dataset to such a degree as to not degrade model performance, when it is used to augment either subsets of the data or the entire training data - in which case it may be even beneficial.

A \textbf{strong} caveat here is, again, the fact that this data was used to create the architecture so information leakage can hardly be avoided.

It is also possible that models trained on a boosted dataset learned to ignore the boosted data by identifying it from learning the patterns created by the generator. 

This is only speculation, but an indicator seems to be the fact that in experiments with significantly larger data boosts (up to +20x, see \ref{fig:ex5}) model performance did not decrease, but with complete replacement (see \ref{fig:replacing_titanic}) model performance did in fact decrease significantly, if less so than with other datasets. 

Although as noted in the section on replacement conclusions, part of the effect, specifically on \acp{nn} is most likely due to the nature of the training process

\pagebreak

A passable conjecture here is a composite effect of multiple individual effects, that is: 

\begin{itemize}
	\item performance deterioration due to the training specifics used in the \ac{nn} as seen here \ref{fig:training_logs_tensorboard}
	\item differences in representation power between real and synthetic training data
	\item the fact that gridsearch, by searching a large hyperparameter space, is uniquely likely to result in information leakage (and ignoring synthetic training data)
\end{itemize}

The fact remains, that on the original dataset, with a large amount of manual \ac{GAN} design and searching for ideal hyperparameters, in the right circumstances \textbf{slight improvements} may be possible.

\pagebreak

\section{Conclusions from application to other datasets}

The entire approach of selecting a mini-battery of models using default parameters and applying them to datasets without any feature engineering with and without boosted data seemed essential after the amount of time, architecture optimization (of the generator) and processing power (on finding the ideal model architecture) that had been applied to the original dataset.

After all of this, there was simply no way that information leakage (and by extension, overfitting on the test data) had not occurred, \textbf{especially} since the dataset in question was small by design and the model applied to it had a fairly outsized amount of representational power. 

This was fairly intentional since the explicit goal was to evaluate whether using more complex models (i.e. \acp{nn}) on such datasets by increasing their size was a feasible trade-off against feature engineering. This would have required a gain in performance, however.

Thankfully, a large and diverse amount of machine learning datasets are now publicly available, and the additional work of selecting new datasets as a "blank canvas" to perform a sanity check on is tractable. 

As a side note, before the explosion of machine learning popularity and the resulting immense increase in availability of resources online this would not have been possible at all, simply due to the fact that sufficiently analogous datasets were not to be found.

\pagebreak

In terms of applying the modules that were developed to completely new data (without any configuration or adaptation of the architecture of the \ac{GAN}) the result could not have been more promising. 

The generator logs that are included (for wine classification here \ref{fig:wasserstein_wine} and diabetes here \ref{fig:wasserstein_diabetes}) were genuinely produced with the finished data enhancement module completely out of the box. 

Especially after the initial difficulties of creating any architecture that would converge on vector data at all, and the resulting effort involved in creating the training loop and adding functionality for Wasserstein-GP style generator training, to say this was promising would be an understatement.

The training logs for new data did show some instability though, and from the detailed results of the standard models applied to unchanged and boosted data of these datasets, some patterns seem to have emerged:

\begin{itemize}
	\item the degree to which the dataset is \textbf{analogous} to the original dataset makes a significant difference. Specifically the fact that the wine quality dataset has a class balance of roughly 1-4 in contrast to the titanic and diabetes datasets with roughly 1-2 seemed to have a large impact
	\item any data from an automatic \ac{GAN} architecture that is not well tuned to the specific training data will immediately degrade performance
\end{itemize} 

Concerning the second part, I \textbf{suspect} that this has to do specifically with the ratio of the size of the latent space to the attributes of the training data and the starting number of neurons (before the first upsampling in the generator).

This is purely speculation at this point, but for the initial architecture convergence was completely impossible until the latent space was set at a standard dimension of (8,). The titanic dataset has 9 attributes and the initial layer of the generator has 32 neurons by default. The fact that there is a good degree of symmetry here may be a contributing factor.

\label{note:hypothesis}

This hypothesis would be difficult to confirm though, without finding a large number of similar and dissimilar datasets and then evaluating the approach on them. Even then, the effect might be too small to completely confirm or deny it with confidence.

\pagebreak

\section{Conclusions from complete data replacement}

As discussed, there are plenty of more proven methods of synthetic data generation, some of which offer model performance which is fairly consistent with the original data already, as discussed here \cite{ares_utility}.

While using this method of data generation to create anonymous data was decidedly not the goal from the start, training models on completely synthetic data as mentioned in the paper offers a useful benchmark whether or not the generated data is actually meaningfully reflective of the original data at all on its' own. 

If we consider the extraction of a model out of a training data set as an efficient representation of the information contained in the data with regard to the target variable, the degradation of the predictive power of the same model with respect to the target variable on synthetic data can be constructed as a useful indicator whether or not the actual meaning of the data has been preserved.

Also, and even more acutely relevant, after performing gridsearch on the original data and training models on synthetically increased datasets other than titanic, it was still unclear whether the models trained on such synthetic data retained some predictive power simply due to the degree to which they managed to identify (and ignore) the synthetic data and thereby effectively acting in part like a pseudo-discriminator.

Concerning the specific results of completely synthetic data model performance, I suspect that this is likely - especially considering the models that preserved the most predictive power with partly synthetic data were random forest models, which are notoriously robust (due to them being ensemble models).

\pagebreak

Perhaps most interestingly, all things considered, is the fact that the diabetes dataset experiences the smallest relative loss of predictive power (here: \ref{fig:all_replaced_diabetes}), in relative terms, when data has been completely replaced by synthetic data. Naively, it would make more sense to see the dataset the architecture was originally fit on to experience the smallest loss (actual loss seen here: \ref{fig:replacing_titanic}).

While the fact that data generation with a such a \ac{GAN} is an intriguing option, especially since it does not rely on any assessment of similarity to generated data in an iterative fashion as the \ac{SDV} described by \cite{patki2016synthetic} or the \ac{DS} described by \cite{ping2017datasynthesizer} and is a much more unsupervised method, its' results are not in any way comparable in terms of the preservation of predictive power of models trained on the synthetically generated data. 

In the future, there may be a way to use conditional \acp{GAN} as described by \cite{conditional_gans} to achieve the best of both worlds.

\pagebreak

\section{Final Thoughts}

Ultimately, some parts of the approach taken show promise and may even be reused, specifically:

\begin{itemize}
	\item the \ac{GAN} architecture pattern using symmetrical up-downsampling
	\item the Wasserstein-GP training loop
	\item the \ac{nn} gridsearch
\end{itemize}

However, the key issue seems to be the fit between the exact \ac{GAN} architecture and the specific dataset.

The fact that \ac{GAN} training is not only notoriously difficult but also difficult to automate and the quality of vector data generated by them may not be feasible to be automatically assessed, completely automating this approach using the guidelines proposed by \cite{buitinck2013api} in scikit-learn is probably not feasible.

The purpose of this paper was mainly to show the trade-off between data scientist time investment (in feature engineering, data enrichment and - worst case - even data labeling) and increased processing power; i.e. using outsized representational power and computation to draw increased meaning from an otherwise unchanged dataset, as it is done in approaches like \ac{bagging}.

What appears to be the effect here however, is that the time investment using this approach would - in the ideal case - mainly shift the work expended from feature engineering to \ac{GAN} architecture search.

In cases where only a small amount of data is available and increasing the training dataset it otherwise impossible \textbf{and} feature engineering and data enrichment has been completely exhausted, the approach may hold some promise. Whether or not the ideal architecture may be found dynamically (as mentioned here \ref{note:hypothesis}) remains an open question.

As for now, there appears to be no free lunch available.

\pagebreak
